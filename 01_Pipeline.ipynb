{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "21ut_OAuCW6E"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import spacy\n",
        "import glob\n",
        "import os\n",
        "import re\n",
        "from timeit import default_timer as timer\n",
        "from datetime import timedelta\n",
        "from tqdm.auto import tqdm\n",
        "import itertools\n",
        "import json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEyuNawelMKQ"
      },
      "source": [
        "# 00 - Install spaCy Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TIJpWHFHEiaZ",
        "outputId": "13bc9ae0-344e-4841-95a6-c52ad1d92483"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting de-core-news-sm==3.2.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.2.0/de_core_news_sm-3.2.0-py3-none-any.whl (19.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.1/19.1 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.3.0,>=3.2.0 in /usr/local/lib/python3.9/site-packages (from de-core-news-sm==3.2.0) (3.2.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (1.8.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (1.0.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (21.3)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (2.0.6)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /usr/local/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (8.0.13)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (60.10.0)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (1.0.1)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (0.6.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (2.4.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (0.7.6)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (2.0.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (3.0.3)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /usr/local/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (3.0.9)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (3.0.6)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (4.62.3)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (3.3.0)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (0.4.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (1.21.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (2.26.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (0.9.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.9/site-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (3.0.6)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.9/site-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (5.2.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (4.0.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (3.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (1.26.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (2021.10.8)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (2.0.9)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.9/site-packages (from typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (8.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/site-packages (from jinja2->spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (2.1.0)\n",
            "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('de_core_news_sm')\n"
          ]
        }
      ],
      "source": [
        "#!python -m spacy download de_core_news_sm\n",
        "\n",
        "# previous code used within google colab, following code for usage in different environment\n",
        "\n",
        "import sys\n",
        "!{sys.executable} -m spacy download de_core_news_sm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVfk3CCG3hkQ"
      },
      "source": [
        "# 00 - Define Paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "tzakekhhC12U"
      },
      "outputs": [],
      "source": [
        "# directory containing the raw data files\n",
        "path_data = 'data/training/unlabeled_data/raw/tsv/'\n",
        "\n",
        "# directory for all processed files \n",
        "path_processed_data = 'data/training/unlabeled_data/processed/'\n",
        "\n",
        "# directory containing gold data\n",
        "path_gold_data = 'data/gold/'\n",
        "\n",
        "# directory containing the seed entity lists from knowledge base\n",
        "path_classes = 'data/training/class_lists/'\n",
        "\n",
        "# directory for all extraction lists (original/lemmatized) (unlabeled/labeled)\n",
        "path_found = 'data/training/unlabeled_data/processed/extract/'\n",
        "\n",
        "# directory for annotated training data\n",
        "path_training_data = 'data/training/training_data/'\n",
        "\n",
        "# path for lexicon data\n",
        "path_lexicon = 'data/lexicon/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIXsJp59lEHf"
      },
      "source": [
        "# 00 - Tool Methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "j3y9JFLGCuQS"
      },
      "outputs": [],
      "source": [
        "def cleanup(row):\n",
        "    return row.replace('(', '').replace(')', '').replace('\\\"', '').replace('“', '').replace('„', '').replace('usw.', 'und so weiter').replace('sog.', 'sogenanntes').replace('usf.', 'und so fort').replace('z.B.', 'zum Beispiel') \n",
        "                                                                                                                                                        \n",
        "def split(row):\n",
        "    return row.split()\n",
        "\n",
        "def split_and_explode(input):\n",
        "    return(input['0'].apply(split).explode())\n",
        "\n",
        "def lemmatize(row):\n",
        "    doc = nlp(row)\n",
        "    lemma = \" \".join(token.lemma_ for token in doc)\n",
        "    return lemma\n",
        "\n",
        "def tokenize(row):\n",
        "    doc = nlp(row)\n",
        "    return \" \".join(token.text for token in doc)\n",
        "\n",
        "# labels entity according to IOB2 format\n",
        "def labeler(row, key):\n",
        "    l = len(row.split())\n",
        "    return [key+1 if i !=0 else key for i in range(l)]\n",
        "\n",
        "# used to extract seed entities from unlabeled data\n",
        "def extract(input, group):\n",
        "    result = input['token'].str.extractall(f'({group})')\n",
        "    return result\n",
        "\n",
        "# used to split and label extracted seed entities\n",
        "def label(input, key):\n",
        "    i = input.columns.get_loc('0')\n",
        "    df2 = input['0'].apply(split)\n",
        "    spl = pd.concat([input.iloc[:, :i], df2, input.iloc[:, i+1:]], axis=1).explode('0')\n",
        "    spl.columns = ['sentence_id', 'match', 'token']\n",
        "    spl = spl[['sentence_id', 'token']]\n",
        "    input = input['0'].apply(labeler, key=key).explode()\n",
        "    input = input.rename('label')\n",
        "    spl = pd.concat([spl, input], axis=1)\n",
        "    spl = spl[['sentence_id', 'token', 'label']]\n",
        "    return spl\n",
        "\n",
        "# used to annotate unlabeled training data\n",
        "def merge_text_label(text, label, on):\n",
        "  label = label.drop_duplicates(subset=['sentence_id', 'token'])\n",
        "  text = pd.merge(text, label, on=on, how='left')\n",
        "  if len(text.columns) > 3:\n",
        "      text['label'] = text['label_x'].fillna(text['label_y'])\n",
        "      text = text.rename(columns={'sentence_id_x': 'sentence_id'})\n",
        "      text = text[['sentence_id', 'token', 'label']]\n",
        "  return text\n",
        "\n",
        "\n",
        "# heuristic approach to correct labels based on most frequently found tokens with incorrect labels, may be manually extended\n",
        "def correct_labels(input, c):\n",
        "  len_old = len(input)\n",
        "  if c == 'EO_POL':\n",
        "    indexNames = input[input['token'].isin(['Deutschland', 'Bundeswehr', 'Bayern'])].index\n",
        "    input = input.drop(indexNames)\n",
        "  elif c == 'P_SOZ':\n",
        "    indexNames = input[input['token'].isin(['Arbeit', 'Macht', 'Zivilgesellschaft', 'Leid', 'Vermieter', 'Eigentümer', 'Vermietern', 'Kolonialismus', 'Kapitalismus', 'Elite', 'Eliten', 'Dominanz', 'Legitimität'])].index\n",
        "    input = input.drop(indexNames)\n",
        "  elif c == 'EO_MEDIA':\n",
        "    indexNames = input[input['token'].isin(['Welt'])].index\n",
        "    input = input.drop(indexNames)\n",
        "  elif c == 'P_FUNC':\n",
        "    indexNames = input[input['token'].isin(['Bundeskanzlerin', 'Unternehmer', 'Wissenschaftler', 'Unternehmern', 'Wissenschaftlern'])].index\n",
        "    input = input.drop(indexNames)\n",
        "  elif c == 'EO_MOV':\n",
        "    indexNames = input[input['token'].isin(['Reichsbürger'])].index\n",
        "    input = input.drop(indexNames)\n",
        "  elif c == 'P_ETH':\n",
        "    indexNames = input[input['token'].isin(['Russen'])].index\n",
        "    input = input.drop(indexNames)\n",
        "  elif c == 'P_NAT':\n",
        "    indexNames = input[input['token'].isin(['Berliner', 'Pariser'])].index\n",
        "    input = input.drop(indexNames)\n",
        "  print('Removed ' + str(len_old - len(input)) + ' ambiguous/incorrect tokens for class ' + c)\n",
        "  return input\n",
        "\n",
        "def count_found_entities(input):\n",
        "  input = input[input['label'] != 0]\n",
        "  input = input[['token', 'label']].value_counts()\n",
        "  return pd.DataFrame(input)\n",
        "\n",
        "# used to remove incorrect I-labels without correct B-label\n",
        "def remove_inner_labels(row):\n",
        "  label_list = json.loads(row)\n",
        "  new_list = []\n",
        "  inLabel = False\n",
        "  last_Label_start = 0\n",
        "  count_removed_inner = 0\n",
        "  for i in label_list:\n",
        "    if i == 0:\n",
        "      new_list.append(0)\n",
        "      inLabel = False\n",
        "      last_Label = 0\n",
        "    elif i % 2 == 1:\n",
        "      new_list.append(int(i))\n",
        "      inLabel = True\n",
        "      last_Label_start = int(i)\n",
        "    elif inLabel:\n",
        "      if int(i) == last_Label_start + 1:\n",
        "        new_list.append(int(i))\n",
        "      else: \n",
        "        new_list.append(0)\n",
        "        inLabel = False\n",
        "        count_removed_inner += 1\n",
        "    else: \n",
        "      new_list.append(0)\n",
        "      inLabel = False\n",
        "      count_removed_inner += 1\n",
        "  return new_list\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3g5G-19NljIa"
      },
      "source": [
        "# 01 - Preprocessing Unlabeled Data\n",
        "- Check for duplicates in raw data files\n",
        "- Read in all raw data files that are not also contained in the gold data and that do not have duplicates\n",
        "- Save processed data to file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        },
        "id": "8NcwHcszDCI1",
        "outputId": "6fb6eece-1ddf-4b6e-cd1d-401d37365c2e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "++++++++++ Collecting files... ++++++++++\n",
            "25427 files found in data path.\n",
            "268 files found in gold data path.\n",
            "++++++++++ Checking for duplicate file names in data files... ++++++++++\n",
            "116 duplicates found in data based on file name.\n",
            "++++++++++ Reading in data files... ++++++++++\n",
            "Processed 685 paragraphs from 50 data files.\n",
            "Processed 1440 paragraphs from 100 data files.\n",
            "Processed 2273 paragraphs from 150 data files.\n",
            "Processed 2986 paragraphs from 200 data files.\n",
            "Processed 3708 paragraphs from 250 data files.\n",
            "Processed 4074 paragraphs from 300 data files.\n",
            "Processed 4730 paragraphs from 350 data files.\n",
            "Processed 5459 paragraphs from 400 data files.\n",
            "Processed 5843 paragraphs from 450 data files.\n",
            "Processed 6721 paragraphs from 500 data files.\n",
            "Processed 7631 paragraphs from 550 data files.\n",
            "Processed 8325 paragraphs from 600 data files.\n",
            "Processed 8663 paragraphs from 650 data files.\n",
            "Processed 9766 paragraphs from 700 data files.\n",
            "Processed 10592 paragraphs from 750 data files.\n",
            "Processed 11519 paragraphs from 800 data files.\n",
            "Processed 11749 paragraphs from 850 data files.\n",
            "Processed 12376 paragraphs from 900 data files.\n",
            "Processed 13281 paragraphs from 950 data files.\n",
            "Processed 14191 paragraphs from 1000 data files.\n",
            "Processed 15043 paragraphs from 1050 data files.\n",
            "Processed 15406 paragraphs from 1100 data files.\n",
            "Processed 16077 paragraphs from 1150 data files.\n",
            "Processed 16913 paragraphs from 1200 data files.\n",
            "Processed 17741 paragraphs from 1250 data files.\n",
            "Processed 18605 paragraphs from 1300 data files.\n",
            "Processed 19225 paragraphs from 1350 data files.\n",
            "Processed 19912 paragraphs from 1400 data files.\n",
            "Processed 20697 paragraphs from 1450 data files.\n",
            "Processed 21363 paragraphs from 1500 data files.\n",
            "Processed 21669 paragraphs from 1550 data files.\n",
            "Processed 22768 paragraphs from 1600 data files.\n",
            "Processed 23640 paragraphs from 1650 data files.\n",
            "Processed 24435 paragraphs from 1700 data files.\n",
            "Processed 25280 paragraphs from 1750 data files.\n",
            "Processed 26189 paragraphs from 1800 data files.\n",
            "Processed 26429 paragraphs from 1850 data files.\n",
            "Processed 26997 paragraphs from 1900 data files.\n",
            "Processed 27667 paragraphs from 1950 data files.\n",
            "Processed 28501 paragraphs from 2000 data files.\n",
            "Processed 29287 paragraphs from 2050 data files.\n",
            "Processed 30081 paragraphs from 2100 data files.\n",
            "Processed 30322 paragraphs from 2150 data files.\n",
            "Processed 31019 paragraphs from 2200 data files.\n",
            "Processed 31836 paragraphs from 2250 data files.\n",
            "Processed 32708 paragraphs from 2300 data files.\n",
            "Processed 33570 paragraphs from 2350 data files.\n",
            "Processed 34566 paragraphs from 2400 data files.\n",
            "Processed 35664 paragraphs from 2450 data files.\n",
            "Processed 36506 paragraphs from 2500 data files.\n",
            "Processed 37405 paragraphs from 2550 data files.\n",
            "Processed 38256 paragraphs from 2600 data files.\n",
            "Processed 39153 paragraphs from 2650 data files.\n",
            "Processed 39289 paragraphs from 2700 data files.\n",
            "Processed 39978 paragraphs from 2750 data files.\n",
            "Processed 40773 paragraphs from 2800 data files.\n",
            "Processed 41705 paragraphs from 2850 data files.\n",
            "Processed 42488 paragraphs from 2900 data files.\n",
            "Processed 43305 paragraphs from 2950 data files.\n",
            "Processed 43822 paragraphs from 3000 data files.\n",
            "Processed 44148 paragraphs from 3050 data files.\n",
            "Processed 44948 paragraphs from 3100 data files.\n",
            "Processed 45753 paragraphs from 3150 data files.\n",
            "Processed 46573 paragraphs from 3200 data files.\n",
            "Processed 47555 paragraphs from 3250 data files.\n",
            "Processed 47896 paragraphs from 3300 data files.\n",
            "Processed 48407 paragraphs from 3350 data files.\n",
            "Processed 49231 paragraphs from 3400 data files.\n",
            "Processed 50089 paragraphs from 3450 data files.\n",
            "Processed 50893 paragraphs from 3500 data files.\n",
            "Processed 51747 paragraphs from 3550 data files.\n",
            "Processed 52578 paragraphs from 3600 data files.\n",
            "Processed 53540 paragraphs from 3650 data files.\n",
            "Processed 54481 paragraphs from 3700 data files.\n",
            "Processed 55582 paragraphs from 3750 data files.\n",
            "Processed 56534 paragraphs from 3800 data files.\n",
            "Processed 57473 paragraphs from 3850 data files.\n",
            "Processed 58453 paragraphs from 3900 data files.\n",
            "Processed 59249 paragraphs from 3950 data files.\n",
            "Processed 60392 paragraphs from 4000 data files.\n",
            "Processed 61164 paragraphs from 4050 data files.\n",
            "Processed 62097 paragraphs from 4100 data files.\n",
            "Processed 62945 paragraphs from 4150 data files.\n",
            "Processed 63431 paragraphs from 4200 data files.\n",
            "Processed 63916 paragraphs from 4250 data files.\n",
            "Processed 64601 paragraphs from 4300 data files.\n",
            "Processed 65436 paragraphs from 4350 data files.\n",
            "Processed 66225 paragraphs from 4400 data files.\n",
            "Processed 67016 paragraphs from 4450 data files.\n",
            "Processed 67712 paragraphs from 4500 data files.\n",
            "Processed 67855 paragraphs from 4550 data files.\n",
            "Processed 68658 paragraphs from 4600 data files.\n",
            "Processed 69308 paragraphs from 4650 data files.\n",
            "Processed 70067 paragraphs from 4700 data files.\n",
            "Processed 70766 paragraphs from 4750 data files.\n",
            "Processed 71613 paragraphs from 4800 data files.\n",
            "Processed 72285 paragraphs from 4850 data files.\n",
            "Processed 72837 paragraphs from 4900 data files.\n",
            "Processed 73301 paragraphs from 4950 data files.\n",
            "Processed 74021 paragraphs from 5000 data files.\n",
            "Processed 74811 paragraphs from 5050 data files.\n",
            "Processed 75670 paragraphs from 5100 data files.\n",
            "Processed 76444 paragraphs from 5150 data files.\n",
            "Processed 76759 paragraphs from 5200 data files.\n",
            "Processed 77416 paragraphs from 5250 data files.\n",
            "Processed 78208 paragraphs from 5300 data files.\n",
            "Processed 79142 paragraphs from 5350 data files.\n",
            "Processed 79904 paragraphs from 5400 data files.\n",
            "Processed 80565 paragraphs from 5450 data files.\n",
            "Processed 81378 paragraphs from 5500 data files.\n",
            "Processed 82295 paragraphs from 5550 data files.\n",
            "Processed 83413 paragraphs from 5600 data files.\n",
            "Processed 84248 paragraphs from 5650 data files.\n",
            "Processed 85162 paragraphs from 5700 data files.\n",
            "Processed 85988 paragraphs from 5750 data files.\n",
            "Processed 86692 paragraphs from 5800 data files.\n",
            "Processed 86942 paragraphs from 5850 data files.\n",
            "Processed 87456 paragraphs from 5900 data files.\n",
            "Processed 88239 paragraphs from 5950 data files.\n",
            "Processed 89026 paragraphs from 6000 data files.\n",
            "Processed 89904 paragraphs from 6050 data files.\n",
            "Processed 90800 paragraphs from 6100 data files.\n",
            "Processed 91583 paragraphs from 6150 data files.\n",
            "Processed 91716 paragraphs from 6200 data files.\n",
            "Processed 92255 paragraphs from 6250 data files.\n",
            "Processed 93150 paragraphs from 6300 data files.\n",
            "Processed 94028 paragraphs from 6350 data files.\n",
            "Processed 94868 paragraphs from 6400 data files.\n",
            "Processed 95278 paragraphs from 6450 data files.\n",
            "Processed 95740 paragraphs from 6500 data files.\n",
            "Processed 96481 paragraphs from 6550 data files.\n",
            "Processed 97249 paragraphs from 6600 data files.\n",
            "Processed 98157 paragraphs from 6650 data files.\n",
            "Processed 98568 paragraphs from 6700 data files.\n",
            "Processed 98792 paragraphs from 6750 data files.\n",
            "Processed 99536 paragraphs from 6800 data files.\n",
            "Processed 100345 paragraphs from 6850 data files.\n",
            "Processed 101191 paragraphs from 6900 data files.\n",
            "Processed 101988 paragraphs from 6950 data files.\n",
            "Processed 102547 paragraphs from 7000 data files.\n",
            "Processed 103008 paragraphs from 7050 data files.\n",
            "Processed 103856 paragraphs from 7100 data files.\n",
            "Processed 104548 paragraphs from 7150 data files.\n",
            "Processed 105354 paragraphs from 7200 data files.\n",
            "Processed 106107 paragraphs from 7250 data files.\n",
            "Processed 106695 paragraphs from 7300 data files.\n",
            "Processed 107013 paragraphs from 7350 data files.\n",
            "Processed 107776 paragraphs from 7400 data files.\n",
            "Processed 108577 paragraphs from 7450 data files.\n",
            "Processed 109357 paragraphs from 7500 data files.\n",
            "Processed 110243 paragraphs from 7550 data files.\n",
            "Processed 110601 paragraphs from 7600 data files.\n",
            "Processed 110759 paragraphs from 7650 data files.\n",
            "Processed 111478 paragraphs from 7700 data files.\n",
            "Processed 112200 paragraphs from 7750 data files.\n",
            "Processed 112904 paragraphs from 7800 data files.\n",
            "Processed 113710 paragraphs from 7850 data files.\n",
            "Processed 114591 paragraphs from 7900 data files.\n",
            "Processed 114784 paragraphs from 7950 data files.\n",
            "Processed 115107 paragraphs from 8000 data files.\n",
            "Processed 115699 paragraphs from 8050 data files.\n",
            "Processed 116633 paragraphs from 8100 data files.\n",
            "Processed 117568 paragraphs from 8150 data files.\n",
            "Processed 118391 paragraphs from 8200 data files.\n",
            "Processed 118535 paragraphs from 8250 data files.\n",
            "Processed 118957 paragraphs from 8300 data files.\n",
            "Processed 119756 paragraphs from 8350 data files.\n",
            "Processed 120501 paragraphs from 8400 data files.\n",
            "Processed 121357 paragraphs from 8450 data files.\n",
            "Processed 122076 paragraphs from 8500 data files.\n",
            "Processed 122438 paragraphs from 8550 data files.\n",
            "Processed 122633 paragraphs from 8600 data files.\n",
            "Processed 123537 paragraphs from 8650 data files.\n",
            "Processed 124222 paragraphs from 8700 data files.\n",
            "Processed 124963 paragraphs from 8750 data files.\n",
            "Processed 125702 paragraphs from 8800 data files.\n",
            "Processed 126488 paragraphs from 8850 data files.\n",
            "Processed 126861 paragraphs from 8900 data files.\n",
            "Processed 127219 paragraphs from 8950 data files.\n",
            "Processed 127957 paragraphs from 9000 data files.\n",
            "Processed 128730 paragraphs from 9050 data files.\n",
            "Processed 129528 paragraphs from 9100 data files.\n",
            "Processed 130243 paragraphs from 9150 data files.\n",
            "Processed 131031 paragraphs from 9200 data files.\n",
            "Processed 131132 paragraphs from 9250 data files.\n",
            "Processed 131501 paragraphs from 9300 data files.\n",
            "Processed 132199 paragraphs from 9350 data files.\n",
            "Processed 133033 paragraphs from 9400 data files.\n",
            "Processed 133897 paragraphs from 9450 data files.\n",
            "Processed 134791 paragraphs from 9500 data files.\n",
            "Processed 135534 paragraphs from 9550 data files.\n",
            "Processed 135751 paragraphs from 9600 data files.\n",
            "Processed 136106 paragraphs from 9650 data files.\n",
            "Processed 136820 paragraphs from 9700 data files.\n",
            "Processed 137530 paragraphs from 9750 data files.\n",
            "Processed 138338 paragraphs from 9800 data files.\n",
            "Processed 139153 paragraphs from 9850 data files.\n",
            "Processed 139932 paragraphs from 9900 data files.\n",
            "Processed 140146 paragraphs from 9950 data files.\n",
            "Processed 140565 paragraphs from 10000 data files.\n",
            "Processed 141329 paragraphs from 10050 data files.\n",
            "Processed 142186 paragraphs from 10100 data files.\n",
            "Processed 143060 paragraphs from 10150 data files.\n",
            "Processed 143784 paragraphs from 10200 data files.\n",
            "Processed 144498 paragraphs from 10250 data files.\n",
            "Processed 145249 paragraphs from 10300 data files.\n",
            "Processed 146096 paragraphs from 10350 data files.\n",
            "Processed 147032 paragraphs from 10400 data files.\n",
            "Processed 148118 paragraphs from 10450 data files.\n",
            "Processed 148960 paragraphs from 10500 data files.\n",
            "Processed 149812 paragraphs from 10550 data files.\n",
            "Processed 150125 paragraphs from 10600 data files.\n",
            "Processed 150350 paragraphs from 10650 data files.\n",
            "Processed 151008 paragraphs from 10700 data files.\n",
            "Processed 151729 paragraphs from 10750 data files.\n",
            "Processed 152569 paragraphs from 10800 data files.\n",
            "Processed 153430 paragraphs from 10850 data files.\n",
            "Processed 154148 paragraphs from 10900 data files.\n",
            "Processed 154572 paragraphs from 10950 data files.\n",
            "Processed 154720 paragraphs from 11000 data files.\n",
            "Processed 155385 paragraphs from 11050 data files.\n",
            "Processed 156040 paragraphs from 11100 data files.\n",
            "Processed 156781 paragraphs from 11150 data files.\n",
            "Processed 157623 paragraphs from 11200 data files.\n",
            "Processed 158376 paragraphs from 11250 data files.\n",
            "Processed 158841 paragraphs from 11300 data files.\n",
            "Processed 159030 paragraphs from 11350 data files.\n",
            "Processed 159804 paragraphs from 11400 data files.\n",
            "Processed 160587 paragraphs from 11450 data files.\n",
            "Processed 161258 paragraphs from 11500 data files.\n",
            "Processed 162089 paragraphs from 11550 data files.\n",
            "Processed 162856 paragraphs from 11600 data files.\n",
            "Processed 163349 paragraphs from 11650 data files.\n",
            "Processed 163665 paragraphs from 11700 data files.\n",
            "Processed 164409 paragraphs from 11750 data files.\n",
            "Processed 165157 paragraphs from 11800 data files.\n",
            "Processed 165966 paragraphs from 11850 data files.\n",
            "Processed 166822 paragraphs from 11900 data files.\n",
            "Processed 167655 paragraphs from 11950 data files.\n",
            "Processed 167930 paragraphs from 12000 data files.\n",
            "Processed 168456 paragraphs from 12050 data files.\n",
            "Processed 169311 paragraphs from 12100 data files.\n",
            "Processed 170082 paragraphs from 12150 data files.\n",
            "Processed 170996 paragraphs from 12200 data files.\n",
            "Processed 171833 paragraphs from 12250 data files.\n",
            "Processed 172624 paragraphs from 12300 data files.\n",
            "Processed 173481 paragraphs from 12350 data files.\n",
            "Processed 174406 paragraphs from 12400 data files.\n",
            "Processed 175394 paragraphs from 12450 data files.\n",
            "Processed 176251 paragraphs from 12500 data files.\n",
            "Processed 177111 paragraphs from 12550 data files.\n",
            "Processed 177752 paragraphs from 12600 data files.\n",
            "Processed 177991 paragraphs from 12650 data files.\n",
            "Processed 178685 paragraphs from 12700 data files.\n",
            "Processed 179531 paragraphs from 12750 data files.\n",
            "Processed 180192 paragraphs from 12800 data files.\n",
            "Processed 180931 paragraphs from 12850 data files.\n",
            "Processed 181718 paragraphs from 12900 data files.\n",
            "Processed 182313 paragraphs from 12950 data files.\n",
            "Processed 182484 paragraphs from 13000 data files.\n",
            "Processed 183230 paragraphs from 13050 data files.\n",
            "Processed 183950 paragraphs from 13100 data files.\n",
            "Processed 184741 paragraphs from 13150 data files.\n",
            "Processed 185538 paragraphs from 13200 data files.\n",
            "Processed 186320 paragraphs from 13250 data files.\n",
            "Processed 186818 paragraphs from 13300 data files.\n",
            "Processed 187109 paragraphs from 13350 data files.\n",
            "Processed 187766 paragraphs from 13400 data files.\n",
            "Processed 188490 paragraphs from 13450 data files.\n",
            "Processed 189249 paragraphs from 13500 data files.\n",
            "Processed 189982 paragraphs from 13550 data files.\n",
            "Processed 190815 paragraphs from 13600 data files.\n",
            "Processed 191110 paragraphs from 13650 data files.\n",
            "Processed 191251 paragraphs from 13700 data files.\n",
            "Processed 191917 paragraphs from 13750 data files.\n",
            "Processed 192665 paragraphs from 13800 data files.\n",
            "Processed 193417 paragraphs from 13850 data files.\n",
            "Processed 194223 paragraphs from 13900 data files.\n",
            "Processed 194957 paragraphs from 13950 data files.\n",
            "Processed 195066 paragraphs from 14000 data files.\n",
            "Processed 195478 paragraphs from 14050 data files.\n",
            "Processed 196191 paragraphs from 14100 data files.\n",
            "Processed 196930 paragraphs from 14150 data files.\n",
            "Processed 197683 paragraphs from 14200 data files.\n",
            "Processed 198545 paragraphs from 14250 data files.\n",
            "Processed 199114 paragraphs from 14300 data files.\n",
            "Processed 199363 paragraphs from 14350 data files.\n",
            "Processed 200042 paragraphs from 14400 data files.\n",
            "Processed 200770 paragraphs from 14450 data files.\n",
            "Processed 201520 paragraphs from 14500 data files.\n",
            "Processed 202283 paragraphs from 14550 data files.\n",
            "Processed 203012 paragraphs from 14600 data files.\n",
            "Processed 203618 paragraphs from 14650 data files.\n",
            "Processed 203745 paragraphs from 14700 data files.\n",
            "Processed 204341 paragraphs from 14750 data files.\n",
            "Processed 205020 paragraphs from 14800 data files.\n",
            "Processed 205716 paragraphs from 14850 data files.\n",
            "Processed 206492 paragraphs from 14900 data files.\n",
            "Processed 207131 paragraphs from 14950 data files.\n",
            "Processed 207514 paragraphs from 15000 data files.\n",
            "Processed 207699 paragraphs from 15050 data files.\n",
            "Processed 208333 paragraphs from 15100 data files.\n",
            "Processed 209018 paragraphs from 15150 data files.\n",
            "Processed 209477 paragraphs from 15200 data files.\n",
            "Processed 209651 paragraphs from 15250 data files.\n",
            "Processed 210245 paragraphs from 15300 data files.\n",
            "Processed 210982 paragraphs from 15350 data files.\n",
            "Processed 211645 paragraphs from 15400 data files.\n",
            "Processed 212152 paragraphs from 15450 data files.\n",
            "Processed 212325 paragraphs from 15500 data files.\n",
            "Processed 212983 paragraphs from 15550 data files.\n",
            "Processed 213681 paragraphs from 15600 data files.\n",
            "Processed 214314 paragraphs from 15650 data files.\n",
            "Processed 214987 paragraphs from 15700 data files.\n",
            "Processed 215813 paragraphs from 15750 data files.\n",
            "Processed 216581 paragraphs from 15800 data files.\n",
            "Processed 217227 paragraphs from 15850 data files.\n",
            "Processed 217432 paragraphs from 15900 data files.\n",
            "Processed 218081 paragraphs from 15950 data files.\n",
            "Processed 218869 paragraphs from 16000 data files.\n",
            "Processed 219472 paragraphs from 16050 data files.\n",
            "Processed 220184 paragraphs from 16100 data files.\n",
            "Processed 220915 paragraphs from 16150 data files.\n",
            "Processed 221522 paragraphs from 16200 data files.\n",
            "Processed 221659 paragraphs from 16250 data files.\n",
            "Processed 222150 paragraphs from 16300 data files.\n",
            "Processed 222904 paragraphs from 16350 data files.\n",
            "Processed 223522 paragraphs from 16400 data files.\n",
            "Processed 224316 paragraphs from 16450 data files.\n",
            "Processed 225040 paragraphs from 16500 data files.\n",
            "Processed 225837 paragraphs from 16550 data files.\n",
            "Processed 226532 paragraphs from 16600 data files.\n",
            "Processed 226652 paragraphs from 16650 data files.\n",
            "Processed 227111 paragraphs from 16700 data files.\n",
            "Processed 227814 paragraphs from 16750 data files.\n",
            "Processed 228627 paragraphs from 16800 data files.\n",
            "Processed 229382 paragraphs from 16850 data files.\n",
            "Processed 230151 paragraphs from 16900 data files.\n",
            "Processed 230914 paragraphs from 16950 data files.\n",
            "Processed 231704 paragraphs from 17000 data files.\n",
            "Processed 232007 paragraphs from 17050 data files.\n",
            "Processed 232275 paragraphs from 17100 data files.\n",
            "Processed 233063 paragraphs from 17150 data files.\n",
            "Processed 233641 paragraphs from 17200 data files.\n",
            "Processed 234385 paragraphs from 17250 data files.\n",
            "Processed 235157 paragraphs from 17300 data files.\n",
            "Processed 235804 paragraphs from 17350 data files.\n",
            "Processed 236365 paragraphs from 17400 data files.\n",
            "Processed 236536 paragraphs from 17450 data files.\n",
            "Processed 237216 paragraphs from 17500 data files.\n",
            "Processed 237872 paragraphs from 17550 data files.\n",
            "Processed 238466 paragraphs from 17600 data files.\n",
            "Processed 239143 paragraphs from 17650 data files.\n",
            "Processed 239846 paragraphs from 17700 data files.\n",
            "Processed 240534 paragraphs from 17750 data files.\n",
            "Processed 241456 paragraphs from 17800 data files.\n",
            "Processed 242224 paragraphs from 17850 data files.\n",
            "Processed 243349 paragraphs from 17900 data files.\n",
            "Processed 244177 paragraphs from 17950 data files.\n",
            "Processed 244991 paragraphs from 18000 data files.\n",
            "Processed 245895 paragraphs from 18050 data files.\n",
            "Processed 246570 paragraphs from 18100 data files.\n",
            "Processed 246723 paragraphs from 18150 data files.\n",
            "Processed 247474 paragraphs from 18200 data files.\n",
            "Processed 248228 paragraphs from 18250 data files.\n",
            "Processed 248905 paragraphs from 18300 data files.\n",
            "Processed 249651 paragraphs from 18350 data files.\n",
            "Processed 250334 paragraphs from 18400 data files.\n",
            "Processed 251021 paragraphs from 18450 data files.\n",
            "Processed 251368 paragraphs from 18500 data files.\n",
            "Processed 251759 paragraphs from 18550 data files.\n",
            "Processed 252541 paragraphs from 18600 data files.\n",
            "Processed 253218 paragraphs from 18650 data files.\n",
            "Processed 253911 paragraphs from 18700 data files.\n",
            "Processed 254894 paragraphs from 18750 data files.\n",
            "Processed 255616 paragraphs from 18800 data files.\n",
            "Processed 256094 paragraphs from 18850 data files.\n",
            "Processed 256334 paragraphs from 18900 data files.\n",
            "Processed 256991 paragraphs from 18950 data files.\n",
            "Processed 257695 paragraphs from 19000 data files.\n",
            "Processed 258323 paragraphs from 19050 data files.\n",
            "Processed 259050 paragraphs from 19100 data files.\n",
            "Processed 259894 paragraphs from 19150 data files.\n",
            "Processed 260600 paragraphs from 19200 data files.\n",
            "Processed 260854 paragraphs from 19250 data files.\n",
            "Processed 261342 paragraphs from 19300 data files.\n",
            "Processed 262118 paragraphs from 19350 data files.\n",
            "Processed 262739 paragraphs from 19400 data files.\n",
            "Processed 263451 paragraphs from 19450 data files.\n",
            "Processed 264204 paragraphs from 19500 data files.\n",
            "Processed 264733 paragraphs from 19550 data files.\n",
            "Processed 264880 paragraphs from 19600 data files.\n",
            "Processed 265650 paragraphs from 19650 data files.\n",
            "Processed 266411 paragraphs from 19700 data files.\n",
            "Processed 267039 paragraphs from 19750 data files.\n",
            "Processed 267851 paragraphs from 19800 data files.\n",
            "Processed 268548 paragraphs from 19850 data files.\n",
            "Processed 269461 paragraphs from 19900 data files.\n",
            "Processed 270325 paragraphs from 19950 data files.\n",
            "Processed 271349 paragraphs from 20000 data files.\n",
            "Processed 272132 paragraphs from 20050 data files.\n",
            "Processed 272930 paragraphs from 20100 data files.\n",
            "Processed 273737 paragraphs from 20150 data files.\n",
            "Processed 273911 paragraphs from 20200 data files.\n",
            "Processed 274430 paragraphs from 20250 data files.\n",
            "Processed 275089 paragraphs from 20300 data files.\n",
            "Processed 275901 paragraphs from 20350 data files.\n",
            "Processed 276693 paragraphs from 20400 data files.\n",
            "Processed 277278 paragraphs from 20450 data files.\n",
            "Processed 277466 paragraphs from 20500 data files.\n",
            "Processed 278102 paragraphs from 20550 data files.\n",
            "Processed 278770 paragraphs from 20600 data files.\n",
            "Processed 279501 paragraphs from 20650 data files.\n",
            "Processed 279966 paragraphs from 20700 data files.\n",
            "Processed 280104 paragraphs from 20750 data files.\n",
            "Processed 280579 paragraphs from 20800 data files.\n",
            "Processed 281275 paragraphs from 20850 data files.\n",
            "Processed 281888 paragraphs from 20900 data files.\n",
            "Processed 282587 paragraphs from 20950 data files.\n",
            "Processed 283384 paragraphs from 21000 data files.\n",
            "Processed 284147 paragraphs from 21050 data files.\n",
            "Processed 284584 paragraphs from 21100 data files.\n",
            "Processed 284909 paragraphs from 21150 data files.\n",
            "Processed 285527 paragraphs from 21200 data files.\n",
            "Processed 286305 paragraphs from 21250 data files.\n",
            "Processed 287073 paragraphs from 21300 data files.\n",
            "Processed 287749 paragraphs from 21350 data files.\n",
            "Processed 288489 paragraphs from 21400 data files.\n",
            "Processed 288637 paragraphs from 21450 data files.\n",
            "Processed 289273 paragraphs from 21500 data files.\n",
            "Processed 290020 paragraphs from 21550 data files.\n",
            "Processed 290771 paragraphs from 21600 data files.\n",
            "Processed 291457 paragraphs from 21650 data files.\n",
            "Processed 292095 paragraphs from 21700 data files.\n",
            "Processed 292247 paragraphs from 21750 data files.\n",
            "Processed 292831 paragraphs from 21800 data files.\n",
            "Processed 293605 paragraphs from 21850 data files.\n",
            "Processed 294296 paragraphs from 21900 data files.\n",
            "Processed 294940 paragraphs from 21950 data files.\n",
            "Processed 295651 paragraphs from 22000 data files.\n",
            "Processed 296377 paragraphs from 22050 data files.\n",
            "Processed 296996 paragraphs from 22100 data files.\n",
            "Processed 297225 paragraphs from 22150 data files.\n",
            "Processed 297891 paragraphs from 22200 data files.\n",
            "Processed 298557 paragraphs from 22250 data files.\n",
            "Processed 299187 paragraphs from 22300 data files.\n",
            "Processed 299866 paragraphs from 22350 data files.\n",
            "Processed 300617 paragraphs from 22400 data files.\n",
            "Processed 301333 paragraphs from 22450 data files.\n",
            "Processed 301628 paragraphs from 22500 data files.\n",
            "Processed 301939 paragraphs from 22550 data files.\n",
            "Processed 302700 paragraphs from 22600 data files.\n",
            "Processed 303432 paragraphs from 22650 data files.\n",
            "Processed 304032 paragraphs from 22700 data files.\n",
            "Processed 304605 paragraphs from 22750 data files.\n",
            "Processed 305327 paragraphs from 22800 data files.\n",
            "Processed 306119 paragraphs from 22850 data files.\n",
            "Processed 306301 paragraphs from 22900 data files.\n",
            "Processed 306794 paragraphs from 22950 data files.\n",
            "Processed 307423 paragraphs from 23000 data files.\n",
            "Processed 308199 paragraphs from 23050 data files.\n",
            "Processed 308886 paragraphs from 23100 data files.\n",
            "Processed 309572 paragraphs from 23150 data files.\n",
            "Processed 310255 paragraphs from 23200 data files.\n",
            "Processed 310961 paragraphs from 23250 data files.\n",
            "Processed 311155 paragraphs from 23300 data files.\n",
            "Processed 311626 paragraphs from 23350 data files.\n",
            "Processed 312413 paragraphs from 23400 data files.\n",
            "Processed 313057 paragraphs from 23450 data files.\n",
            "Processed 313719 paragraphs from 23500 data files.\n",
            "Processed 314432 paragraphs from 23550 data files.\n",
            "Processed 315036 paragraphs from 23600 data files.\n",
            "Processed 315790 paragraphs from 23650 data files.\n",
            "Processed 315937 paragraphs from 23700 data files.\n",
            "Processed 316446 paragraphs from 23750 data files.\n",
            "Processed 317166 paragraphs from 23800 data files.\n",
            "Processed 317860 paragraphs from 23850 data files.\n",
            "Processed 318546 paragraphs from 23900 data files.\n",
            "Processed 319215 paragraphs from 23950 data files.\n",
            "Processed 320055 paragraphs from 24000 data files.\n",
            "Processed 320636 paragraphs from 24050 data files.\n",
            "Processed 321152 paragraphs from 24100 data files.\n",
            "Processed 321314 paragraphs from 24150 data files.\n",
            "Processed 322078 paragraphs from 24200 data files.\n",
            "Processed 322824 paragraphs from 24250 data files.\n",
            "Processed 323526 paragraphs from 24300 data files.\n",
            "Processed 324171 paragraphs from 24350 data files.\n",
            "Processed 324853 paragraphs from 24400 data files.\n",
            "Processed 325430 paragraphs from 24450 data files.\n",
            "Processed 325776 paragraphs from 24500 data files.\n",
            "Processed 326267 paragraphs from 24550 data files.\n",
            "Processed 326930 paragraphs from 24600 data files.\n",
            "Processed 327638 paragraphs from 24650 data files.\n",
            "Processed 328466 paragraphs from 24700 data files.\n",
            "Processed 329278 paragraphs from 24750 data files.\n",
            "Processed 330019 paragraphs from 24800 data files.\n",
            "Processed 330856 paragraphs from 24850 data files.\n",
            "Processed 332094 paragraphs from 24900 data files.\n",
            "Processed 332689 paragraphs from 24950 data files.\n",
            "Processed 333453 paragraphs from 25000 data files.\n",
            "Processed 334176 paragraphs from 25050 data files.\n",
            "Read in 334276 paragraphs from 25057 unlabeled data files, excluding 116 duplicate files and 255 files also found in gold data.\n",
            "Duration: 0:03:51.848023\n",
            "Saved paragraphs in directory: data/training/unlabeled_data/processed/\n",
            "Max Sentence length = 335\n"
          ]
        }
      ],
      "source": [
        "print('++++++++++ Collecting files... ++++++++++')\n",
        "\n",
        "all_data_files = glob.glob(os.path.join(path_data, '*.csv'))\n",
        "all_gold_data_files = glob.glob(os.path.join(path_gold_data, '*.csv'))\n",
        "\n",
        "\n",
        "print(str(len(all_data_files)) + ' files found in data path.')\n",
        "print(str(len(all_gold_data_files)) + ' files found in gold data path.')\n",
        "\n",
        "print('++++++++++ Checking for duplicate file names in data files... ++++++++++')\n",
        "\n",
        "count_dups = 0\n",
        "all_data_files.sort()\n",
        "for k, g in itertools.groupby(all_data_files, lambda f: os.path.splitext(f)[0]):\n",
        "     duplicates = list(g)\n",
        "     if len(duplicates) > 1:\n",
        "        count_dups +=1\n",
        "print(str(count_dups) + ' duplicates found in data based on file name.')\n",
        "\n",
        "\n",
        "count_files = 0\n",
        "count_sentences = 0\n",
        "max_length = 0\n",
        "input = []\n",
        "read_files = []\n",
        "count_coliding = 0\n",
        "\n",
        "print('++++++++++ Reading in data files... ++++++++++')\n",
        "start = timer()\n",
        "\n",
        "for f in all_data_files:\n",
        "    mock_file_name_gold = path_gold_data + f.replace(path_data, '')\n",
        "    if mock_file_name_gold  in all_gold_data_files:\n",
        "        count_coliding +=1\n",
        "    \n",
        "    # only process files not also contained in gold set\n",
        "    if f not in read_files and mock_file_name_gold not in all_gold_data_files:\n",
        "        read_files.append(f)\n",
        "        count_files += 1\n",
        "        file = open(f, 'r')\n",
        "        Lines = file.readlines()\n",
        "\n",
        "        for line in Lines:\n",
        "\n",
        "            # extract the raw text from csv files\n",
        "            if line.startswith('#Text='):\n",
        "                line = line.replace('#Text=', '')\n",
        "                input.append(line)\n",
        "                count_sentences += 1\n",
        "\n",
        "                if len(line.split()) > max_length:\n",
        "                    max_length = len(line.split())\n",
        "        \n",
        "        if count_files % 50 == 0:\n",
        "            print('Processed ' + str(count_sentences) + ' paragraphs from ' + str(count_files) + ' data files.')\n",
        "    \n",
        "    \n",
        "data = pd.DataFrame({'token' : input})\n",
        "data.to_csv(path_processed_data+'processed_data.csv', index=False)\n",
        "\n",
        "end = timer()\n",
        "\n",
        "print('Read in ' + str(count_sentences) + ' paragraphs from ' + str(count_files) + ' unlabeled data files, excluding ' + str(count_dups) + ' duplicate files and ' + str(count_coliding) + ' files also found in gold data.')\n",
        "print('Duration: ' + str(timedelta(seconds=end - start)))\n",
        "print('Saved paragraphs in directory: ' + str(path_processed_data))\n",
        "print(\"Max Sentence length = \" + str(max_length))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Itd1DRM2lvSu"
      },
      "source": [
        "# 02 A - Tokenizing/Lemmatizing Unlabeled Data\n",
        "- Tokenize the unlabeled data\n",
        "- Create a second version of the unlabeled data, lemmatize this second version\n",
        "- Save both versions to files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 914,
          "referenced_widgets": [
            "c3ab738d1513478cb0963b406624831d",
            "ea3d49ff563442fb8919b475bb05cbf0",
            "f949f184b5d04853a86d3fdcbb3ca7fe",
            "9c487735b9c84b8e9707e442552e3476",
            "5b2f0489be184f2696c9744530ad68b5",
            "73958a2143e64052b16908e7af530ca4",
            "8474896ba19c49d886883c8561a50796",
            "5c87632296624fc78b71f8a82e1b2f7d",
            "3ea78bff89b142dbb454a476bda51ae9",
            "51dba03ba8c8495ca675a9f74af5e226",
            "fb84f72a0cea42b083cccdca84c6654b",
            "6a6bb2605c3d4faf978fd318d6e4e55e",
            "8f24188cfba347b3a01f2385456e6d21",
            "d5cf9cc6b5f340e392b434671c024c0e",
            "2a392235e69442c29da94f08d54bf37d",
            "14bec54aa19d4082bc432c8fc2a2b2cb",
            "c44305f42df3435f9145c6dc30adbe07",
            "89ffc130f5b14dc8b474150f2c34970c",
            "33177bf842ee4ff98da93440a6eb78d0",
            "7b2037c837d94d1a8734e7d8ac458cf9",
            "3e70c976c0bf439c8f126ea4d704bbb5",
            "a8f0291a60c34fa0a110fa9c162a869b"
          ]
        },
        "id": "yKUXENiwDMS1",
        "outputId": "d978d066-366b-4bbd-85ea-578858e5b359"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "++++++++++ Reading in processed data ... ++++++++++\n",
            "                                                    token\n",
            "0       Guten Morgen, liebe Kolleginnen und Kollegen! ...\n",
            "1       Meine sehr verehrten Damen und Herren! Liebe K...\n",
            "2       § 1 Absatz 2 der Geschäftsordnung des Deutsche...\n",
            "3       Die Fraktion der AfD widerspricht diesem Verfa...\n",
            "4       Enthaltungen? – Der Antrag ist damit mit den S...\n",
            "...                                                   ...\n",
            "334271  Der letzte Punkt; Frau Deligöz hat es gerade a...\n",
            "334272  mit der Begründung, die Bundesregierung sei ja...\n",
            "334273  An die Kommunen und an die Familien sende ich ...\n",
            "334274                                 Herzlichen Dank.\\n\n",
            "334275  Vielen Dank, Frau Kollegin. – Als letzter Redn...\n",
            "\n",
            "[334276 rows x 1 columns]\n",
            "++++++++++ Loading spaCy model ... ++++++++++\n",
            "++++++++++ Cleaning data ... ++++++++++\n",
            "++++++++++ Tokenizing data ... ++++++++++\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c3ab738d1513478cb0963b406624831d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Tokenizing data:   0%|          | 0/334276 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                                    token\n",
            "0       Guten Morgen , liebe Kolleginnen und Kollegen ...\n",
            "1       Meine sehr verehrten Damen und Herren ! Liebe ...\n",
            "2       § 1 Absatz 2 der Geschäftsordnung des Deutsche...\n",
            "3       Die Fraktion der AfD widerspricht diesem Verfa...\n",
            "4       Enthaltungen ? – Der Antrag ist damit mit den ...\n",
            "...                                                   ...\n",
            "334271  Der letzte Punkt ; Frau Deligöz hat es gerade ...\n",
            "334272  mit der Begründung , die Bundesregierung sei j...\n",
            "334273  An die Kommunen und an die Familien sende ich ...\n",
            "334274                               Herzlichen Dank . \\n\n",
            "334275  Vielen Dank , Frau Kollegin . – Als letzter Re...\n",
            "\n",
            "[334276 rows x 1 columns]\n",
            "Saved tokenized data set to directory: ./drive/MyDrive/model/data/training/unlabeled_data/processed/\n",
            "++++++++++ Creating lemmatized data set ... ++++++++++\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6a6bb2605c3d4faf978fd318d6e4e55e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Lemmatizing data:   0%|          | 0/334276 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                                    token\n",
            "0       Guten Morgen , lieb Kollegin und Kollege ! Neh...\n",
            "1       Meine sehr verehren Dame und Herr ! lieben Kol...\n",
            "2       § 1 Absatz 2 der Geschäftsordnung der Deutsche...\n",
            "3       der Fraktion der AfD widersprechen dies Verfah...\n",
            "4       Enthaltung ? – der Antrag sein damit mit der S...\n",
            "...                                                   ...\n",
            "334271  der letzte Punkt ; Frau Deligöz haben ich gera...\n",
            "334272  mit der Begründung , der Bundesregierung sein ...\n",
            "334273  an der Kommune und an der Familie senden ich a...\n",
            "334274                                Herzliche Dank . \\n\n",
            "334275  viel Dank , Frau Kollegin . – als letzt Redner...\n",
            "\n",
            "[334276 rows x 1 columns]\n",
            "Saved lemmatized data set to directory: ./drive/MyDrive/model/data/training/unlabeled_data/processed/lemmatized/\n"
          ]
        }
      ],
      "source": [
        "print('++++++++++ Reading in processed data ... ++++++++++')\n",
        "data = pd.read_csv(path_processed_data + 'processed_data.csv')\n",
        "print(data)\n",
        "\n",
        "\n",
        "print('++++++++++ Loading spaCy model ... ++++++++++')\n",
        "nlp = spacy.load('de_core_news_sm', disable=['tagger', 'parser', 'ner'])\n",
        "\n",
        "print('++++++++++ Cleaning data ... ++++++++++')\n",
        "data['token'] = data['token'].apply(cleanup)\n",
        "data = data[['token']]\n",
        "\n",
        "print('++++++++++ Tokenizing data ... ++++++++++')\n",
        "tqdm.pandas(desc='Tokenizing Data')\n",
        "data['token'] = data['token'].progress_apply(tokenize)\n",
        "print(data)\n",
        "data.to_csv(path_processed_data + 'data_tokenized.csv', index=False)\n",
        "print('Saved tokenized data set: ' + path_processed_data + 'data_tokenized.csv')\n",
        "\n",
        "print('++++++++++ Creating lemmatized data set ... ++++++++++')\n",
        "data_lem = data.copy()\n",
        "tqdm.pandas (desc='Lemmatizing data')\n",
        "data_lem['token'] = data_lem['token'].progress_apply(lemmatize)\n",
        "print(data_lem)\n",
        "data_lem.to_csv(path_processed_data + 'lemmatized/data_lemmatized.csv', index=False)\n",
        "print('Saved lemmatized data set: ' + path_processed_data + 'lemmatized/data_lemmatized.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OI4xckcTZay0"
      },
      "source": [
        "# 02 B - Reading in Tokenized and Lemmatized Data\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31DOf23IDULF",
        "outputId": "61f8292e-71d1-4b22-92ec-fd3d0ad4ff44"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                                    token\n",
            "0       Guten Morgen , liebe Kolleginnen und Kollegen ...\n",
            "1       Meine sehr verehrten Damen und Herren ! Liebe ...\n",
            "2       § 1 Absatz 2 der Geschäftsordnung des Deutsche...\n",
            "3       Die Fraktion der AfD widerspricht diesem Verfa...\n",
            "4       Enthaltungen ? – Der Antrag ist damit mit den ...\n",
            "...                                                   ...\n",
            "334271  Der letzte Punkt ; Frau Deligöz hat es gerade ...\n",
            "334272  mit der Begründung , die Bundesregierung sei j...\n",
            "334273  An die Kommunen und an die Familien sende ich ...\n",
            "334274                               Herzlichen Dank . \\n\n",
            "334275  Vielen Dank , Frau Kollegin . – Als letzter Re...\n",
            "\n",
            "[334276 rows x 1 columns]\n",
            "                                                    token\n",
            "0       Guten Morgen , lieb Kollegin und Kollege ! Neh...\n",
            "1       Meine sehr verehren Dame und Herr ! lieben Kol...\n",
            "2       § 1 Absatz 2 der Geschäftsordnung der Deutsche...\n",
            "3       der Fraktion der AfD widersprechen dies Verfah...\n",
            "4       Enthaltung ? – der Antrag sein damit mit der S...\n",
            "...                                                   ...\n",
            "334271  der letzte Punkt ; Frau Deligöz haben ich gera...\n",
            "334272  mit der Begründung , der Bundesregierung sein ...\n",
            "334273  an der Kommune und an der Familie senden ich a...\n",
            "334274                                Herzliche Dank . \\n\n",
            "334275  viel Dank , Frau Kollegin . – als letzt Redner...\n",
            "\n",
            "[334276 rows x 1 columns]\n"
          ]
        }
      ],
      "source": [
        "data = pd.read_csv(path_processed_data + 'data_tokenized.csv')\n",
        "data_lem = pd.read_csv(path_processed_data + 'lemmatized/data_lemmatized.csv')\n",
        "print(data)\n",
        "print(data_lem)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bu5-d3PhlTrf"
      },
      "source": [
        "# 03 - Processing Class Files\n",
        "- Read in the seed entities extracted from Wikidata\n",
        "- Create regex for each class\n",
        "- Collect seed entities for seed lexicon"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r81sV68SC8HP",
        "outputId": "33722e11-d396-4d87-fd0c-296a32634bb8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "++++++++++ Processing class files... ++++++++++\n",
            "Processed 491 final sample entities for class 1 : EO_SCI\n",
            "Dropped 9 entities with missing labels for class 1 : EO_SCI\n",
            "Processed 500 final sample entities for class 2 : EP_WIRT\n",
            "Dropped 0 entities with missing labels for class 2 : EP_WIRT\n",
            "Processed 493 final sample entities for class 3 : EP_MIL\n",
            "Dropped 7 entities with missing labels for class 3 : EP_MIL\n",
            "Processed 170 final sample entities for class 4 : EP_MOV\n",
            "Dropped 9 entities with missing labels for class 4 : EP_MOV\n",
            "Processed 483 final sample entities for class 5 : EP_NGO\n",
            "Dropped 17 entities with missing labels for class 5 : EP_NGO\n",
            "Processed 490 final sample entities for class 6 : EP_KULT\n",
            "Dropped 10 entities with missing labels for class 6 : EP_KULT\n",
            "Processed 491 final sample entities for class 7 : EP_SCI\n",
            "Dropped 8 entities with missing labels for class 7 : EP_SCI\n",
            "Processed 489 final sample entities for class 8 : EP_FINANZ\n",
            "Dropped 11 entities with missing labels for class 8 : EP_FINANZ\n",
            "Processed 500 final sample entities for class 9 : EP_MEDIA\n",
            "Dropped 0 entities with missing labels for class 9 : EP_MEDIA\n",
            "Processed 475 final sample entities for class 10 : EP_REL\n",
            "Dropped 25 entities with missing labels for class 10 : EP_REL\n",
            "Processed 500 final sample entities for class 11 : EP_POL\n",
            "Dropped 0 entities with missing labels for class 11 : EP_POL\n",
            "Processed 479 final sample entities for class 12 : EO_POL\n",
            "Dropped 19 entities with missing labels for class 12 : EO_POL\n",
            "Processed 457 final sample entities for class 13 : EO_MIL\n",
            "Dropped 41 entities with missing labels for class 13 : EO_MIL\n",
            "Processed 499 final sample entities for class 14 : EO_WIRT\n",
            "Dropped 1 entities with missing labels for class 14 : EO_WIRT\n",
            "Processed 252 final sample entities for class 15 : EO_MEDIA\n",
            "Dropped 2 entities with missing labels for class 15 : EO_MEDIA\n",
            "Processed 495 final sample entities for class 16 : EO_KULT\n",
            "Dropped 5 entities with missing labels for class 16 : EO_KULT\n",
            "Processed 233 final sample entities for class 17 : EO_REL\n",
            "Dropped 6 entities with missing labels for class 17 : EO_REL\n",
            "Processed 423 final sample entities for class 18 : EO_MOV\n",
            "Dropped 76 entities with missing labels for class 18 : EO_MOV\n",
            "Processed 429 final sample entities for class 19 : EO_NGO\n",
            "Dropped 70 entities with missing labels for class 19 : EO_NGO\n",
            "Processed 497 final sample entities for class 20 : P_FUNC\n",
            "Dropped 0 entities with missing labels for class 20 : P_FUNC\n",
            "Processed 5 final sample entities for class 21 : P_GEN\n",
            "Dropped 0 entities with missing labels for class 21 : P_GEN\n",
            "Processed 300 final sample entities for class 22 : EO_FINANZ\n",
            "Dropped 0 entities with missing labels for class 22 : EO_FINANZ\n",
            "Processed 158 final sample entities for class 23 : P_SOZ\n",
            "Dropped 0 entities with missing labels for class 23 : P_SOZ\n",
            "Processed 22 final sample entities for class 24 : P_AGE\n",
            "Dropped 0 entities with missing labels for class 24 : P_AGE\n",
            "Processed 12 final sample entities for class 25 : EP_OWN\n",
            "Dropped 0 entities with missing labels for class 25 : EP_OWN\n",
            "Processed 498 final sample entities for class 26 : P_ETH\n",
            "Dropped 0 entities with missing labels for class 26 : P_ETH\n",
            "Processed 181 final sample entities for class 27 : P_NAT\n",
            "Dropped 0 entities with missing labels for class 27 : P_NAT\n",
            "Processed 262 final sample entities for class 28 : GPE\n",
            "Dropped 4 entities with missing labels for class 28 : GPE\n",
            "++++++++++ Finished processing class files... ++++++++++\n"
          ]
        }
      ],
      "source": [
        "print('++++++++++ Processing class files... ++++++++++')\n",
        "\n",
        "class_list = {}\n",
        "count_entities = 0\n",
        "count_faults = 0\n",
        "count_classes = 1\n",
        "\n",
        "# lists for seed lexicon\n",
        "lex_ents = []\n",
        "lex_labs = []\n",
        "\n",
        "all_class_files = glob.glob(os.path.join(path_classes, '*.csv'))\n",
        "\n",
        "for cf in all_class_files:\n",
        "    cfile = open(cf, 'r')\n",
        "    cLines = cfile.readlines()\n",
        "    cLabel = cf.replace('.csv', '').replace(path_classes, '')\n",
        "\n",
        "    entity_string = ''\n",
        "\n",
        "    \n",
        "    for line in cLines:\n",
        "        line = cleanup(line)\n",
        "        if line.startswith('itemLabel'):\n",
        "          # ignore irrelevant lines\n",
        "            continue\n",
        "        elif re.match('Q\\d+', line):\n",
        "          # ignore irrelevant lines\n",
        "            count_faults += 1\n",
        "            continue\n",
        "        else:\n",
        "          # process seed entities\n",
        "            entity_string += line.replace('\\n', '') + '|'\n",
        "            lex_ents.append(line.replace('\\n', ''))\n",
        "            lex_labs.append(cLabel)\n",
        "            count_entities += 1\n",
        "\n",
        "    # save regular expression for each class\n",
        "    class_list[cLabel] = entity_string[:-1]\n",
        "    print('Processed ' + str(count_entities) + ' final sample entities for class ' + str(count_classes) + ' : ' + cLabel)\n",
        "    print('Dropped ' + str(count_faults) + ' entities with missing labels for class ' + str(count_classes) + ' : ' + cLabel)\n",
        "    print('')\n",
        "\n",
        "    count_entities = 0\n",
        "    count_faults = 0\n",
        "    count_classes += 1\n",
        "\n",
        "\n",
        "print(\"++++++++++ Finished processing class files... ++++++++++\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYWQE4fUpNgh"
      },
      "source": [
        "# 04 - Create Seed Lexicon \n",
        "- Create seed lexicon from processed class lists\n",
        "- save seed lexicon to file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6lbydXGlpNKB",
        "outputId": "207c555e-89bc-4c34-9b88-3aa9930cc47f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "EP_WIRT      500\n",
              "EP_POL       500\n",
              "EP_MEDIA     500\n",
              "EO_WIRT      499\n",
              "P_ETH        498\n",
              "P_FUNC       497\n",
              "EO_KULT      495\n",
              "EP_MIL       493\n",
              "EP_SCI       491\n",
              "EO_SCI       491\n",
              "EP_KULT      490\n",
              "EP_FINANZ    489\n",
              "EP_NGO       483\n",
              "EO_POL       479\n",
              "EP_REL       475\n",
              "EO_MIL       457\n",
              "EO_NGO       429\n",
              "EO_MOV       423\n",
              "EO_FINANZ    300\n",
              "GPE          262\n",
              "EO_MEDIA     252\n",
              "EO_REL       233\n",
              "P_NAT        181\n",
              "EP_MOV       170\n",
              "P_SOZ        158\n",
              "P_AGE         22\n",
              "EP_OWN        12\n",
              "P_GEN          5\n",
              "Name: label, dtype: int64"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "seed_lex = pd.DataFrame({'entity' : lex_ents, 'label' : lex_labs})\n",
        "seed_lex = seed_lex.sort_values(['label'])\n",
        "seed_lex.to_csv(path_lexicon + 'seed_lex.csv', index = False)\n",
        "print('Saved seed lexicon to directory: ' + path_lexicon)\n",
        "\n",
        "# count number of entities per class in seed lexicon\n",
        "seed_lex.label.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4aPyHOpnlz8K"
      },
      "source": [
        "# 05 A - Identifying and Extracting Instances of Seed Entities in Unlabeled Data\n",
        "- Use previously defined regex to extract instances of seed entities from both versions of the unlabeled data\n",
        "- save extracted seed entities to files, one file per class and version (original/tokenized)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_WcyQOH3DmxE",
        "outputId": "15cec3cb-aa53-47c0-d41f-44a6529be2b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "++++++++++ Extracting entity samples in data ... ++++++++++\n",
            "Extracting class: 1 EO_SCI from normal data...\n",
            "EO_SCI -> 284 entities found.\n",
            "Extracting class: 2 EP_WIRT from normal data...\n",
            "EP_WIRT -> 38 entities found.\n",
            "Extracting class: 3 EP_MIL from normal data...\n",
            "EP_MIL -> 548 entities found.\n",
            "Extracting class: 4 EP_MOV from normal data...\n",
            "EP_MOV -> 7 entities found.\n",
            "Extracting class: 5 EP_NGO from normal data...\n",
            "EP_NGO -> 1453 entities found.\n",
            "Extracting class: 6 EP_KULT from normal data...\n",
            "EP_KULT -> 196 entities found.\n",
            "Extracting class: 7 EP_SCI from normal data...\n",
            "EP_SCI -> 418 entities found.\n",
            "Extracting class: 8 EP_FINANZ from normal data...\n",
            "EP_FINANZ -> 204 entities found.\n",
            "Extracting class: 9 EP_MEDIA from normal data...\n",
            "EP_MEDIA -> 289 entities found.\n",
            "Extracting class: 10 EP_REL from normal data...\n",
            "EP_REL -> 17 entities found.\n",
            "Extracting class: 11 EP_POL from normal data...\n",
            "EP_POL -> 12596 entities found.\n",
            "Extracting class: 12 EO_POL from normal data...\n",
            "EO_POL -> 50336 entities found.\n",
            "Extracting class: 13 EO_MIL from normal data...\n",
            "EO_MIL -> 487 entities found.\n",
            "Extracting class: 14 EO_WIRT from normal data...\n",
            "EO_WIRT -> 7550 entities found.\n",
            "Extracting class: 15 EO_MEDIA from normal data...\n",
            "EO_MEDIA -> 19164 entities found.\n",
            "Extracting class: 16 EO_KULT from normal data...\n",
            "EO_KULT -> 35 entities found.\n",
            "Extracting class: 17 EO_REL from normal data...\n",
            "EO_REL -> 1475 entities found.\n",
            "Extracting class: 18 EO_MOV from normal data...\n",
            "EO_MOV -> 3256 entities found.\n",
            "Extracting class: 19 EO_NGO from normal data...\n",
            "EO_NGO -> 420 entities found.\n",
            "Extracting class: 20 P_FUNC from normal data...\n",
            "P_FUNC -> 6774 entities found.\n",
            "Extracting class: 21 P_GEN from normal data...\n",
            "P_GEN -> 56741 entities found.\n",
            "Extracting class: 22 EO_FINANZ from normal data...\n",
            "EO_FINANZ -> 27 entities found.\n",
            "Extracting class: 23 P_SOZ from normal data...\n",
            "P_SOZ -> 32726 entities found.\n",
            "Extracting class: 24 P_AGE from normal data...\n",
            "P_AGE -> 23975 entities found.\n",
            "Extracting class: 25 EP_OWN from normal data...\n",
            "EP_OWN -> 927703 entities found.\n",
            "Extracting class: 26 GPE from normal data...\n",
            "GPE -> 60969 entities found.\n",
            "Extracting class: 27 P_ETH from normal data...\n",
            "P_ETH -> 124921 entities found.\n",
            "Extracting class: 28 P_NAT from normal data...\n",
            "P_NAT -> 43682 entities found.\n",
            "Extracting class: 1 EO_SCI from lemmatized data...\n",
            "EO_SCI -> 216 entities found.\n",
            "Extracting class: 2 EP_WIRT from lemmatized data...\n",
            "EP_WIRT -> 38 entities found.\n",
            "Extracting class: 3 EP_MIL from lemmatized data...\n",
            "EP_MIL -> 545 entities found.\n",
            "Extracting class: 4 EP_MOV from lemmatized data...\n",
            "EP_MOV -> 7 entities found.\n",
            "Extracting class: 5 EP_NGO from lemmatized data...\n",
            "EP_NGO -> 1412 entities found.\n",
            "Extracting class: 6 EP_KULT from lemmatized data...\n",
            "EP_KULT -> 195 entities found.\n",
            "Extracting class: 7 EP_SCI from lemmatized data...\n",
            "EP_SCI -> 412 entities found.\n",
            "Extracting class: 8 EP_FINANZ from lemmatized data...\n",
            "EP_FINANZ -> 200 entities found.\n",
            "Extracting class: 9 EP_MEDIA from lemmatized data...\n",
            "EP_MEDIA -> 275 entities found.\n",
            "Extracting class: 10 EP_REL from lemmatized data...\n",
            "EP_REL -> 16 entities found.\n",
            "Extracting class: 11 EP_POL from lemmatized data...\n",
            "EP_POL -> 11992 entities found.\n",
            "Extracting class: 12 EO_POL from lemmatized data...\n",
            "EO_POL -> 40547 entities found.\n",
            "Extracting class: 13 EO_MIL from lemmatized data...\n",
            "EO_MIL -> 492 entities found.\n",
            "Extracting class: 14 EO_WIRT from lemmatized data...\n",
            "EO_WIRT -> 7576 entities found.\n",
            "Extracting class: 15 EO_MEDIA from lemmatized data...\n",
            "EO_MEDIA -> 19026 entities found.\n",
            "Extracting class: 16 EO_KULT from lemmatized data...\n",
            "EO_KULT -> 26 entities found.\n",
            "Extracting class: 17 EO_REL from lemmatized data...\n",
            "EO_REL -> 1478 entities found.\n",
            "Extracting class: 18 EO_MOV from lemmatized data...\n",
            "EO_MOV -> 3156 entities found.\n",
            "Extracting class: 19 EO_NGO from lemmatized data...\n",
            "EO_NGO -> 316 entities found.\n",
            "Extracting class: 20 P_FUNC from lemmatized data...\n",
            "P_FUNC -> 6993 entities found.\n",
            "Extracting class: 21 P_GEN from lemmatized data...\n",
            "P_GEN -> 24878 entities found.\n",
            "Extracting class: 22 EO_FINANZ from lemmatized data...\n",
            "EO_FINANZ -> 27 entities found.\n",
            "Extracting class: 23 P_SOZ from lemmatized data...\n",
            "P_SOZ -> 32909 entities found.\n",
            "Extracting class: 24 P_AGE from lemmatized data...\n",
            "P_AGE -> 23143 entities found.\n",
            "Extracting class: 25 EP_OWN from lemmatized data...\n",
            "EP_OWN -> 1775432 entities found.\n",
            "Extracting class: 26 GPE from lemmatized data...\n",
            "GPE -> 60005 entities found.\n",
            "Extracting class: 27 P_ETH from lemmatized data...\n",
            "P_ETH -> 103047 entities found.\n",
            "Extracting class: 28 P_NAT from lemmatized data...\n",
            "P_NAT -> 39020 entities found.\n",
            "Found 1376291 entities in normal data... \n",
            "Found 2153379 entities in lemmatized data... \n",
            "Duration: 0:38:51.723492\n",
            "Saved extracted entities to directory: ./drive/MyDrive/model/data/training/unlabeled_data/processed/extract/\n",
            "++++++++++ Finished extracting found entity samples in data ... ++++++++++\n"
          ]
        }
      ],
      "source": [
        "# estimated duration: 40 min\n",
        "print('++++++++++ Extracting entity samples in data ... ++++++++++')\n",
        "\n",
        "extracted_lists = {}\n",
        "extracted_lists_lem = {}\n",
        "count_findings = {}\n",
        "count_findings_lem = {}\n",
        "count_class = 1\n",
        "\n",
        "start = timer()\n",
        "\n",
        "for c, l in class_list.items():\n",
        "    print('Extracting class ' + str(count_class) + ': ' + str(c) +  ' from normal data...')\n",
        "\n",
        "    # extract seed entities from original unlabeled data with regular expression\n",
        "    res = extract(data, str(l))\n",
        "\n",
        "    # save extracted entities \n",
        "    res.to_csv(path_found + str(c) + '_extracted_normal.csv', index=True)\n",
        "\n",
        "    extracted_lists[c] = res\n",
        "    print('' + str(c) + ' -> ' + str(len(extracted_lists[c])) + ' entities found.')\n",
        "    count_findings[c] = len(extracted_lists[c])\n",
        "    count_class += 1\n",
        "\n",
        "count_class = 1\n",
        "for c, l in class_list.items():\n",
        "    print('Extracting class: ' + str(count_class) + ' ' + str(c) +  ' from lemmatized data...')\n",
        "\n",
        "    # extract seed entities from lemmatized unlabeled data with regular expression\n",
        "    res = extract(data_lem, str(l))\n",
        "\n",
        "    # save extracted entities\n",
        "    res.to_csv(path_found + str(c) + '_extracted_lemma.csv', index=True)\n",
        "    \n",
        "    extracted_lists_lem[c] = res\n",
        "    print('' + str(c) + ' -> ' + str(len(extracted_lists_lem[c])) + ' entities found.')\n",
        "    count_findings_lem[c] = len(extracted_lists_lem[c])\n",
        "    count_class += 1\n",
        "\n",
        "end = timer()\n",
        "\n",
        "print('Found ' + str(sum(v for v in count_findings.values())) + ' entities in normal data... ')\n",
        "print('Found ' + str(sum(v for v in count_findings_lem.values())) + ' entities in lemmatized data... ')\n",
        "print('Duration: ' + str(timedelta(seconds=end - start)))\n",
        "\n",
        "print('Saved extracted entities to directory: ' + path_found)\n",
        "print('++++++++++ Finished extracting found entity samples in data ... ++++++++++')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bxJPNAyl78b"
      },
      "source": [
        "# 05 B - Loading Instance Extractions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4iZsXHpcfvKK",
        "outputId": "74aac596-05f9-4aa1-a1f6-7f66eefb0d1a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "++++++++++ Reading in extracted entity samples ... ++++++++++\n"
          ]
        }
      ],
      "source": [
        "print(\"++++++++++ Reading in extracted entity samples ... ++++++++++\")\n",
        "\n",
        "all_found_entities_files = glob.glob(os.path.join(path_found, '*_extracted_normal.csv'))\n",
        "all_found_entities_lemmas_files = glob.glob(os.path.join(path_found, '*_extracted_lemma.csv'))\n",
        "\n",
        "extracted_lists = {}\n",
        "extracted_lists_lem = {}\n",
        "\n",
        "for f in all_found_entities_files:\n",
        "  df = pd.read_csv(f)\n",
        "  c = f.replace('_extracted_normal.csv', '').replace(path_found, '')\n",
        "  extracted_lists[c] = df\n",
        "\n",
        "for f in all_found_entities_lemmas_files:\n",
        "  df = pd.read_csv(f)\n",
        "  c = f.replace('_extracted_lemma.csv', '').replace(path_found, '')\n",
        "  extracted_lists_lem[c] = df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRlGzprn5lbw"
      },
      "source": [
        "# 06 - Splitting Unlabeled Data\n",
        "- split paragraphs into separate tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RfEcQQvTbwcn",
        "outputId": "43cfb52e-f299-4ee7-8a7c-ee5c6c048ab5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "++++++++++ Splitting data into tokens ... ++++++++++\n",
            "                   token  sentence_id\n",
            "0                  Guten            0\n",
            "0                 Morgen            0\n",
            "0                      ,            0\n",
            "0                  liebe            0\n",
            "0            Kolleginnen            0\n",
            "...                  ...          ...\n",
            "334275               die       334275\n",
            "334275  CDU/CSU-Fraktion       334275\n",
            "334275               das       334275\n",
            "334275              Wort       334275\n",
            "334275                 .       334275\n",
            "\n",
            "[16459859 rows x 2 columns]\n",
            "                   token  sentence_id\n",
            "0                  Guten            0\n",
            "0                 Morgen            0\n",
            "0                      ,            0\n",
            "0                   lieb            0\n",
            "0               Kollegin            0\n",
            "...                  ...          ...\n",
            "334275               der       334275\n",
            "334275  CDU/CSU-Fraktion       334275\n",
            "334275               der       334275\n",
            "334275              Wort       334275\n",
            "334275                 .       334275\n",
            "\n",
            "[16459859 rows x 2 columns]\n"
          ]
        }
      ],
      "source": [
        "print(\"++++++++++ Splitting data into tokens ... ++++++++++\")\n",
        "\n",
        "data = pd.DataFrame(data['token'].apply(split).explode())\n",
        "data_lem = pd.DataFrame(data_lem['token'].apply(split).explode())\n",
        "data[\"sentence_id\"] = data.index\n",
        "data_lem[\"sentence_id\"] = data_lem.index\n",
        "print(data)\n",
        "print(data_lem)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2CQEysTm2Wf"
      },
      "source": [
        "# (Compare tokenized and lemmatized paragraph length)\n",
        "- Annotation method requires identical length of both data sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GsmVEd2MLBRM",
        "outputId": "d525e3e9-aa8f-4f4a-c417-f1e573589318"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "        normal  lem\n",
            "295194     385  385\n",
            "286652     335  335\n",
            "294668     327  327\n",
            "76607      326  326\n",
            "39603      325  325\n",
            "...        ...  ...\n",
            "9099         1    1\n",
            "20860        1    1\n",
            "141545       1    1\n",
            "186419       1    1\n",
            "154359       1    1\n",
            "\n",
            "[334276 rows x 2 columns]\n",
            "Empty DataFrame\n",
            "Columns: [normal, lem]\n",
            "Index: []\n",
            "['Es', 'reicht', 'auch', 'nicht', ',', 'zu', 'sagen', ':', 'Das', 'war', 'der', 'Nationalsozialismus', ',', 'und', 'das', 'machen', 'wir', 'wieder', 'gut', '.', '–', 'Denn', 'es', 'ist', 'gefordert', ',', 'Selbstkritik', 'zu', 'üben', 'und', 'auf', 'uns', 'selbst', 'zu', 'blicken', '.', 'Einer', 'der', 'bekanntesten', 'Widerstandskämpfer', 'gegen', 'den', 'Nationalsozialismus', 'war', 'der', 'Sozialdemokrat', 'Wilhelm', 'Leuschner', '.', 'Zur', 'Wahrheit', 'gehört', 'aber', 'auch', ',', 'dass', 'in', 'seine', 'Zeit', 'als', 'hessischer', 'Innenminister', 'das', 'Gesetz', 'zur', 'Bekämpfung', '–', 'und', 'ich', 'verwende', 'bewusst', 'nicht', 'den', 'Begriff', '–', 'des', 'Z', '…', '-Unwesens', 'fiel', '.', 'Das', 'heißt', ',', 'der', 'Antiziganismus', 'ist', 'nicht', 'eine', 'Frage', 'der', 'anderen', 'und', 'der', 'Nationalsozialisten', ',', 'sondern', 'es', 'ist', 'eine', 'Frage', 'von', 'uns', 'allen', ',', 'auch', 'von', 'uns', 'Demokratinnen', 'und', 'Demokraten', '.', 'Wenn', 'wir', 'das', 'nicht', 'begreifen', ',', 'werden', 'wir', 'niemals', 'den', 'Sinti', 'und', 'Roma', 'in', 'diesem', 'Land', 'Gerechtigkeit', 'widerfahren', 'lassen', '.']\n",
            "['ich', 'reichen', 'auch', 'nicht', ',', 'zu', 'sagen', ':', 'der', 'sein', 'der', 'Nationalsozialismus', ',', 'und', 'der', 'machen', 'ich', 'wieder', 'gut', '.', '–', 'denn', 'ich', 'sein', 'fordern', ',', 'Selbstkritik', 'zu', 'üben', 'und', 'auf', 'sich', 'selbst', 'zu', 'blicken', '.', 'Einer', 'der', 'bekannt', 'Widerstandskämpfer', 'gegen', 'der', 'Nationalsozialismus', 'sein', 'der', 'Sozialdemokrat', 'Wilhelm', 'Leuschner', '.', 'Zur', 'Wahrheit', 'hören', 'aber', 'auch', ',', 'dass', 'in', 'mein', 'Zeit', 'als', 'hessisch', 'Innenminister', 'der', 'Gesetz', 'zur', 'Bekämpfung', '–', 'und', 'ich', 'verwenden', 'bewusst', 'nicht', 'der', 'Begriff', '–', 'der', 'Z', '…', '-Unwesens', 'fallen', '.', 'der', 'heißen', ',', 'der', 'Antiziganismus', 'sein', 'nicht', 'einen', 'Frage', 'der', 'ander', 'und', 'der', 'Nationalsozialist', ',', 'sondern', 'ich', 'sein', 'einen', 'Frage', 'von', 'sich', 'alle', ',', 'auch', 'von', 'sich', 'Demokratinnen', 'und', 'Demokrat', '.', 'Wenn', 'ich', 'der', 'nicht', 'begreifen', ',', 'werden', 'ich', 'niemals', 'der', 'Sinti', 'und', 'Roma', 'in', 'dies', 'Land', 'Gerechtigkeit', 'widerfahren', 'lassen', '.']\n"
          ]
        }
      ],
      "source": [
        "test = data.sentence_id.value_counts()\n",
        "test_lem = data_lem.sentence_id.value_counts()\n",
        "both = pd.DataFrame(test)\n",
        "both['lem'] = test_lem\n",
        "both.columns = ['normal', 'lem']\n",
        "\n",
        "print(both)\n",
        "print(both[both.normal != both.lem])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jxm13tvhnTH"
      },
      "source": [
        "# 07 A - Split and Label Extracted Instances\n",
        "- split extracted instances (see 05 A) and label them with numeric labels\n",
        "- save labeled/extracted entities to files, one file per class and version (original/tokenized)\n",
        "- for further usage: clean entities based on heuristics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M6dF_mRebs2s",
        "outputId": "4e7c9692-f2b6-45b1-f281-81b4785faa3f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "++++++++++ Create labels for found entities ... ++++++++++\n",
            "Labeled found entities in normal data for class: EO_SCI\n",
            "     sentence_id                     token label\n",
            "0            980  Friedrich-Ebert-Stiftung    31\n",
            "1           1440                Initiative    31\n",
            "1           1440                      Neue    32\n",
            "1           1440                   Soziale    32\n",
            "1           1440           Marktwirtschaft    32\n",
            "..           ...                       ...   ...\n",
            "281       325332            Weltwirtschaft    32\n",
            "282       325616  Friedrich-Ebert-Stiftung    31\n",
            "283       331709          Potsdam-Institut    31\n",
            "283       331709                       für    32\n",
            "283       331709      Klimafolgenforschung    32\n",
            "\n",
            "[682 rows x 3 columns]\n",
            "Labeled found entities in normal data for class: EP_WIRT\n",
            "    sentence_id      token label\n",
            "0         82068     Roland     3\n",
            "0         82068     Baader     4\n",
            "1        100965  Christian     3\n",
            "1        100965        von     4\n",
            "1        100965    Stetten     4\n",
            "..          ...        ...   ...\n",
            "36       329057        von     4\n",
            "36       329057    Stetten     4\n",
            "37       329115  Christian     3\n",
            "37       329115        von     4\n",
            "37       329115    Stetten     4\n",
            "\n",
            "[85 rows x 3 columns]\n",
            "Labeled found entities in normal data for class: EP_MIL\n",
            "     sentence_id           token label\n",
            "0            525          Jürgen    15\n",
            "0            525           Hardt    16\n",
            "1            810          Sigmar    15\n",
            "1            810         Gabriel    16\n",
            "2           1094        Johannes    15\n",
            "..           ...             ...   ...\n",
            "545       329875      Guttenberg    16\n",
            "546       330210          Jürgen    15\n",
            "546       330210           Hardt    16\n",
            "547       331327          Arnold    15\n",
            "547       331327  Schwarzenegger    16\n",
            "\n",
            "[1104 rows x 3 columns]\n",
            "Labeled found entities in normal data for class: EP_MOV\n",
            "   sentence_id       token label\n",
            "0        12041   Margarete    19\n",
            "0        12041   Stokowski    20\n",
            "1       113262       Luisa    19\n",
            "1       113262    Neubauer    20\n",
            "2       151889       Luisa    19\n",
            "2       151889    Neubauer    20\n",
            "3       151889       Luisa    19\n",
            "3       151889    Neubauer    20\n",
            "4       289762   Margarete    19\n",
            "4       289762   Stokowski    20\n",
            "5       317050      Rainer    19\n",
            "5       317050      Werner    20\n",
            "5       317050  Fassbinder    20\n",
            "6       319692       Luisa    19\n",
            "6       319692    Neubauer    20\n",
            "Labeled found entities in normal data for class: EP_NGO\n",
            "      sentence_id         token label\n",
            "0               7  Frank-Walter    17\n",
            "0               7    Steinmeier    18\n",
            "1              16         Heike    17\n",
            "1              16      Baehrens    18\n",
            "2              16         Fritz    17\n",
            "...           ...           ...   ...\n",
            "1450       334112          Maas    18\n",
            "1451       334125         Heiko    17\n",
            "1451       334125          Maas    18\n",
            "1452       334140         Heiko    17\n",
            "1452       334140          Maas    18\n",
            "\n",
            "[2908 rows x 3 columns]\n",
            "Labeled found entities in normal data for class: EP_KULT\n",
            "     sentence_id       token label\n",
            "0           4694         Jan    13\n",
            "0           4694  Böhmermann    14\n",
            "1          13915        Tube    13\n",
            "2          16316        Tube    13\n",
            "3          16316        Tube    13\n",
            "..           ...         ...   ...\n",
            "193       317076  Böhmermann    14\n",
            "194       317120         Jan    13\n",
            "194       317120  Böhmermann    14\n",
            "195       319724         Jan    13\n",
            "195       319724  Böhmermann    14\n",
            "\n",
            "[212 rows x 3 columns]\n",
            "Labeled found entities in normal data for class: EP_SCI\n",
            "     sentence_id    token label\n",
            "0             16  Karamba     9\n",
            "0             16    Diaby    10\n",
            "1           3214   Frauke     9\n",
            "1           3214    Petry    10\n",
            "2           3217   Frauke     9\n",
            "..           ...      ...   ...\n",
            "415       327479    Diaby    10\n",
            "416       327936   Frauke     9\n",
            "416       327936    Petry    10\n",
            "417       331408   Frauke     9\n",
            "417       331408    Petry    10\n",
            "\n",
            "[838 rows x 3 columns]\n",
            "Labeled found entities in normal data for class: EP_FINANZ\n",
            "     sentence_id     token label\n",
            "0            507  Emmanuel     5\n",
            "0            507    Macron     6\n",
            "1           1068  Emmanuel     5\n",
            "1           1068    Macron     6\n",
            "2           1089  Emmanuel     5\n",
            "..           ...       ...   ...\n",
            "201       299287   Nüßlein     6\n",
            "202       308752     Georg     5\n",
            "202       308752   Nüßlein     6\n",
            "203       321839     Georg     5\n",
            "203       321839   Nüßlein     6\n",
            "\n",
            "[408 rows x 3 columns]\n",
            "Labeled found entities in normal data for class: EP_MEDIA\n",
            "     sentence_id      token label\n",
            "0            862  Christian     7\n",
            "0            862    Lindner     8\n",
            "1           3197       Imad     7\n",
            "1           3197      Karim     8\n",
            "2           8050  Christian     7\n",
            "..           ...        ...   ...\n",
            "286       333289    Lindner     8\n",
            "287       333725  Christian     7\n",
            "287       333725    Lindner     8\n",
            "288       333732  Christian     7\n",
            "288       333732    Lindner     8\n",
            "\n",
            "[579 rows x 3 columns]\n",
            "Labeled found entities in normal data for class: EP_REL\n",
            "    sentence_id       token label\n",
            "0         14181        Ilse    11\n",
            "0         14181  Junkermann    12\n",
            "1         27503      Ludwig    11\n",
            "1         27503      Schick    12\n",
            "2         29076      Ludwig    11\n",
            "2         29076      Schick    12\n",
            "3         42550        Elke    11\n",
            "3         42550       König    12\n",
            "4         42608        Elke    11\n",
            "4         42608       König    12\n",
            "5         44959       Klaus    11\n",
            "5         44959      Müller    12\n",
            "6         52684  Constantin    11\n",
            "7         75636      Ludwig    11\n",
            "7         75636      Schick    12\n",
            "8         87805  Constantin    11\n",
            "9        260042     Andreas    11\n",
            "9        260042     Nachama    12\n",
            "10       263297       Anton    11\n",
            "10       263297    Losinger    12\n",
            "11       278562       Peter    11\n",
            "11       278562       Neher    12\n",
            "12       290010      Margot    11\n",
            "12       290010     Käßmann    12\n",
            "13       307810  Constantin    11\n",
            "14       314536  Constantin    11\n",
            "15       323344      Pierre    11\n",
            "15       323344       Vogel    12\n",
            "16       330449      Margot    11\n",
            "16       330449     Käßmann    12\n",
            "Labeled found entities in normal data for class: EP_POL\n",
            "       sentence_id     token label\n",
            "0               16     Heike     1\n",
            "0               16  Baehrens     2\n",
            "1               16   Karamba     1\n",
            "1               16     Diaby     2\n",
            "2               16    Sabine     1\n",
            "...            ...       ...   ...\n",
            "12593       334233     Heidt     2\n",
            "12594       334248    Silvia     1\n",
            "12594       334248    Breher     2\n",
            "12595       334263      Ekin     1\n",
            "12595       334263   Deligöz     2\n",
            "\n",
            "[25668 rows x 3 columns]\n",
            "Labeled found entities in normal data for class: EO_POL\n",
            "       sentence_id        token label\n",
            "0                9          mut    23\n",
            "1               25   Bundeswehr    23\n",
            "2               26   Bundeswehr    23\n",
            "3               27   Bundeswehr    23\n",
            "4               62  Deutschland    23\n",
            "...            ...          ...   ...\n",
            "50334       334263       90/Die    24\n",
            "50334       334263       Grünen    24\n",
            "50335       334269      Bündnis    23\n",
            "50335       334269       90/Die    24\n",
            "50335       334269       Grünen    24\n",
            "\n",
            "[62888 rows x 3 columns]\n",
            "Labeled found entities in normal data for class: EO_MIL\n",
            "     sentence_id          token label\n",
            "0            473         Marine    37\n",
            "1            477         Marine    37\n",
            "2            478         Marine    37\n",
            "3            486         Marine    37\n",
            "4            486         Marine    37\n",
            "..           ...            ...   ...\n",
            "484       330221       Kommando    37\n",
            "484       330221  Spezialkräfte    38\n",
            "485       330470      Luftwaffe    37\n",
            "486       331795       Kommando    37\n",
            "486       331795  Spezialkräfte    38\n",
            "\n",
            "[609 rows x 3 columns]\n",
            "Labeled found entities in normal data for class: EO_WIRT\n",
            "      sentence_id token label\n",
            "0              49  ltur    25\n",
            "1              68  ltur    25\n",
            "2             113  ltur    25\n",
            "3             113  ltur    25\n",
            "4             219  ltur    25\n",
            "...           ...   ...   ...\n",
            "7545       333718  ltur    25\n",
            "7546       333963  ltur    25\n",
            "7547       334098   Sto    25\n",
            "7548       334098   Sto    25\n",
            "7549       334234  ltur    25\n",
            "\n",
            "[7648 rows x 3 columns]\n",
            "Labeled found entities in normal data for class: P_NAT\n",
            "       sentence_id    token label\n",
            "0                1  Deutsch    43\n",
            "1                1  Deutsch    43\n",
            "2                2  Deutsch    43\n",
            "3                2  Deutsch    43\n",
            "4                5  Deutsch    43\n",
            "...            ...      ...   ...\n",
            "43677       334127    Polen    43\n",
            "43678       334141    Polen    43\n",
            "43679       334146  Deutsch    43\n",
            "43680       334176  Deutsch    43\n",
            "43681       334176  Deutsch    43\n",
            "\n",
            "[43682 rows x 3 columns]\n",
            "Labeled found entities in normal data for class: EO_MEDIA\n",
            "       sentence_id         token label\n",
            "0               22          Welt    29\n",
            "1               44          Bild    29\n",
            "2               67           Der    29\n",
            "2               67  Tagesspiegel    30\n",
            "3              147       Phoenix    29\n",
            "...            ...           ...   ...\n",
            "19159       334234          Bild    29\n",
            "19160       334255          Bild    29\n",
            "19161       334269          Bild    29\n",
            "19162       334269          Bild    29\n",
            "19163       334269          Bild    29\n",
            "\n",
            "[19777 rows x 3 columns]\n",
            "Labeled found entities in normal data for class: EO_KULT\n",
            "    sentence_id           token label\n",
            "0         22482        Stiftung    35\n",
            "0         22482     Preußischer    36\n",
            "0         22482    Kulturbesitz    36\n",
            "1         35285  Kulturstiftung    35\n",
            "1         35285             des    36\n",
            "..          ...             ...   ...\n",
            "33       292078     Preußischer    36\n",
            "33       292078    Kulturbesitz    36\n",
            "34       307892  Kulturstiftung    35\n",
            "34       307892             des    36\n",
            "34       307892          Bundes    36\n",
            "\n",
            "[75 rows x 3 columns]\n",
            "Labeled found entities in normal data for class: EO_REL\n",
            "      sentence_id   token label\n",
            "0              13  Missio    33\n",
            "1             485  Missio    33\n",
            "2             485  Missio    33\n",
            "3             497  Missio    33\n",
            "4             511  Missio    33\n",
            "...           ...     ...   ...\n",
            "1470       328976  Missio    33\n",
            "1471       328978  Missio    33\n",
            "1472       329010  Missio    33\n",
            "1473       330452  Missio    33\n",
            "1474       332623  Missio    33\n",
            "\n",
            "[1477 rows x 3 columns]\n",
            "Labeled found entities in normal data for class: EO_MOV\n",
            "      sentence_id           token label\n",
            "0             549  Menschenrechte    41\n",
            "1             665  Menschenrechte    41\n",
            "2             758  Menschenrechte    41\n",
            "3             781  Menschenrechte    41\n",
            "4             781    Kinderrechte    41\n",
            "...           ...             ...   ...\n",
            "3251       332727  Menschenrechte    41\n",
            "3252       332731  Menschenrechte    41\n",
            "3253       332745  Menschenrechte    41\n",
            "3254       332752  Menschenrechte    41\n",
            "3255       334085  Menschenrechte    41\n",
            "\n",
            "[3680 rows x 3 columns]\n",
            "Labeled found entities in normal data for class: EO_NGO\n",
            "     sentence_id                token label\n",
            "0            550            Sea-Watch    39\n",
            "1           1412                   IG    39\n",
            "1           1412               Metall    40\n",
            "2           1558                   IG    39\n",
            "2           1558               Metall    40\n",
            "..           ...                  ...   ...\n",
            "417       331805                  wir    39\n",
            "417       331805               helfen    40\n",
            "418       331990                  wir    39\n",
            "418       331990               helfen    40\n",
            "419       333196  Verbraucherzentrale    39\n",
            "\n",
            "[714 rows x 3 columns]\n",
            "Labeled found entities in normal data for class: P_FUNC\n",
            "      sentence_id            token label\n",
            "0              23  Bundeskanzlerin    47\n",
            "1              75        Schneider    47\n",
            "2              86        Schneider    47\n",
            "3             148  Bundeskanzlerin    47\n",
            "4             149  Bundeskanzlerin    47\n",
            "...           ...              ...   ...\n",
            "6769       334064  Bundeskanzlerin    47\n",
            "6770       334078  Bundeskanzlerin    47\n",
            "6771       334120           Rhetor    47\n",
            "6772       334246         Erzieher    47\n",
            "6773       334246         Erzieher    47\n",
            "\n",
            "[6774 rows x 3 columns]\n",
            "Labeled found entities in normal data for class: GPE\n",
            "       sentence_id        token label\n",
            "0               62  Deutschland    55\n",
            "1               68  Deutschland    55\n",
            "2              101  Deutschland    55\n",
            "3              154      Jamaika    55\n",
            "4              158  Deutschland    55\n",
            "...            ...          ...   ...\n",
            "60964       334123        Polen    55\n",
            "60965       334127        Polen    55\n",
            "60966       334141        Polen    55\n",
            "60967       334176  Deutschland    55\n",
            "60968       334176  Deutschland    55\n",
            "\n",
            "[61265 rows x 3 columns]\n",
            "Labeled found entities in normal data for class: P_GEN\n",
            "       sentence_id   token label\n",
            "0               16    Volk    53\n",
            "1               21    Volk    53\n",
            "2               35    Volk    53\n",
            "3               35    Volk    53\n",
            "4               39    Volk    53\n",
            "...            ...     ...   ...\n",
            "56736       334265  Bürger    53\n",
            "56737       334266  Bürger    53\n",
            "56738       334268  Bürger    53\n",
            "56739       334268  Bürger    53\n",
            "56740       334269   Leute    53\n",
            "\n",
            "[56741 rows x 3 columns]\n",
            "Labeled found entities in normal data for class: P_ETH\n",
            "        sentence_id token label\n",
            "0                 1    Li    45\n",
            "1                 3   Han    45\n",
            "2                 7    Wa    45\n",
            "3                 7    Wa    45\n",
            "4                16    Li    45\n",
            "...             ...   ...   ...\n",
            "124916       334270    Ga    45\n",
            "124917       334270    Wa    45\n",
            "124918       334272    Li    45\n",
            "124919       334273    Li    45\n",
            "124920       334273    Ga    45\n",
            "\n",
            "[124921 rows x 3 columns]\n",
            "Labeled found entities in normal data for class: EO_FINANZ\n",
            "    sentence_id         token label\n",
            "0          2875       Central    27\n",
            "1         10863  Hannoversche    27\n",
            "2         42793        Munich    27\n",
            "2         42793            Re    28\n",
            "3         42793        Munich    27\n",
            "3         42793            Re    28\n",
            "4         49565        Munich    27\n",
            "4         49565            Re    28\n",
            "5         49565        Munich    27\n",
            "5         49565            Re    28\n",
            "6         49567        Munich    27\n",
            "6         49567            Re    28\n",
            "7         49567        Munich    27\n",
            "7         49567            Re    28\n",
            "8         49569        Munich    27\n",
            "8         49569            Re    28\n",
            "9         55247       Colonia    27\n",
            "10        55255       Colonia    27\n",
            "11        55265       Colonia    27\n",
            "12        55416       Colonia    27\n",
            "13        83655       Colonia    27\n",
            "14        86229       Colonia    27\n",
            "15        88445        Munich    27\n",
            "15        88445            Re    28\n",
            "16       146560       Colonia    27\n",
            "17       172182       Central    27\n",
            "18       178692  Hannoversche    27\n",
            "19       196221       Central    27\n",
            "20       198196  Hannoversche    27\n",
            "21       202659  Hannoversche    27\n",
            "22       251146       Central    27\n",
            "23       251162       Central    27\n",
            "24       257183        Debeka    27\n",
            "25       311725        Debeka    27\n",
            "26       326098       Central    27\n",
            "Labeled found entities in normal data for class: P_SOZ\n",
            "       sentence_id        token label\n",
            "0               36       Arbeit    51\n",
            "1               38        Macht    51\n",
            "2               51       Arbeit    51\n",
            "3               53        Macht    51\n",
            "4               77       Arbeit    51\n",
            "...            ...          ...   ...\n",
            "32721       334127  Geflüchtete    51\n",
            "32722       334130  Geflüchtete    51\n",
            "32723       334135       Arbeit    51\n",
            "32724       334188        Macht    51\n",
            "32725       334249       Arbeit    51\n",
            "\n",
            "[32733 rows x 3 columns]\n",
            "Labeled found entities in normal data for class: P_AGE\n",
            "       sentence_id  token label\n",
            "0                5  Alter    49\n",
            "1                6  Alter    49\n",
            "2                7  Alter    49\n",
            "3               58  Alter    49\n",
            "4               62  Alter    49\n",
            "...            ...    ...   ...\n",
            "23970       334249   Kind    49\n",
            "23971       334249   Kind    49\n",
            "23972       334251   Kind    49\n",
            "23973       334259   Kind    49\n",
            "23974       334269   Kind    49\n",
            "\n",
            "[23978 rows x 3 columns]\n",
            "Labeled found entities in normal data for class: EP_OWN\n",
            "        sentence_id token label\n",
            "0                 1  Mein    21\n",
            "1                 1   Ich    21\n",
            "2                 1   ich    21\n",
            "3                 3   ich    21\n",
            "4                 3   ich    21\n",
            "...             ...   ...   ...\n",
            "927698       334273   ich    21\n",
            "927699       334273   ich    21\n",
            "927700       334273   ich    21\n",
            "927701       334273  mein    21\n",
            "927702       334274   ich    21\n",
            "\n",
            "[927703 rows x 3 columns]\n",
            "Labeled found entities in lemma data for class: EO_SCI\n",
            "     sentence_id                     token label\n",
            "0            980  Friedrich-Ebert-Stiftung    31\n",
            "1           3160                  Stiftung    31\n",
            "1           3160              Wissenschaft    32\n",
            "1           3160                       und    32\n",
            "1           3160                   Politik    32\n",
            "..           ...                       ...   ...\n",
            "213       325332            Weltwirtschaft    32\n",
            "214       325616  Friedrich-Ebert-Stiftung    31\n",
            "215       331709          Potsdam-Institut    31\n",
            "215       331709                       für    32\n",
            "215       331709      Klimafolgenforschung    32\n",
            "\n",
            "[429 rows x 3 columns]\n",
            "Labeled found entities in lemma data for class: EP_WIRT\n",
            "    sentence_id      token label\n",
            "0         82068     Roland     3\n",
            "0         82068     Baader     4\n",
            "1        100965  Christian     3\n",
            "1        100965        von     4\n",
            "1        100965    Stetten     4\n",
            "..          ...        ...   ...\n",
            "36       329057        von     4\n",
            "36       329057    Stetten     4\n",
            "37       329115  Christian     3\n",
            "37       329115        von     4\n",
            "37       329115    Stetten     4\n",
            "\n",
            "[85 rows x 3 columns]\n",
            "Labeled found entities in lemma data for class: EP_MIL\n",
            "     sentence_id           token label\n",
            "0            525          Jürgen    15\n",
            "0            525           Hardt    16\n",
            "1            810          Sigmar    15\n",
            "1            810         Gabriel    16\n",
            "2           1094        Johannes    15\n",
            "..           ...             ...   ...\n",
            "542       329875      Guttenberg    16\n",
            "543       330210          Jürgen    15\n",
            "543       330210           Hardt    16\n",
            "544       331327          Arnold    15\n",
            "544       331327  Schwarzenegger    16\n",
            "\n",
            "[1098 rows x 3 columns]\n",
            "Labeled found entities in lemma data for class: EP_MOV\n",
            "   sentence_id       token label\n",
            "0        12041   Margarete    19\n",
            "0        12041   Stokowski    20\n",
            "1       113262       Luisa    19\n",
            "1       113262    Neubauer    20\n",
            "2       151889       Luisa    19\n",
            "2       151889    Neubauer    20\n",
            "3       151889       Luisa    19\n",
            "3       151889    Neubauer    20\n",
            "4       289762   Margarete    19\n",
            "4       289762   Stokowski    20\n",
            "5       317050      Rainer    19\n",
            "5       317050      Werner    20\n",
            "5       317050  Fassbinder    20\n",
            "6       319692       Luisa    19\n",
            "6       319692    Neubauer    20\n",
            "Labeled found entities in lemma data for class: EP_NGO\n",
            "      sentence_id         token label\n",
            "0               7  Frank-Walter    17\n",
            "0               7    Steinmeier    18\n",
            "1              16         Heike    17\n",
            "1              16      Baehrens    18\n",
            "2              16         Fritz    17\n",
            "...           ...           ...   ...\n",
            "1409       334112          Maas    18\n",
            "1410       334125         Heiko    17\n",
            "1410       334125          Maas    18\n",
            "1411       334140         Heiko    17\n",
            "1411       334140          Maas    18\n",
            "\n",
            "[2826 rows x 3 columns]\n",
            "Labeled found entities in lemma data for class: EP_KULT\n",
            "     sentence_id       token label\n",
            "0           4694         Jan    13\n",
            "0           4694  Böhmermann    14\n",
            "1          13915        Tube    13\n",
            "2          16316        Tube    13\n",
            "3          16316        Tube    13\n",
            "..           ...         ...   ...\n",
            "192       317076  Böhmermann    14\n",
            "193       317120         Jan    13\n",
            "193       317120  Böhmermann    14\n",
            "194       319724         Jan    13\n",
            "194       319724  Böhmermann    14\n",
            "\n",
            "[210 rows x 3 columns]\n",
            "Labeled found entities in lemma data for class: EP_SCI\n",
            "     sentence_id    token label\n",
            "0             16  Karamba     9\n",
            "0             16    Diaby    10\n",
            "1           3214   Frauke     9\n",
            "1           3214    Petry    10\n",
            "2           3217   Frauke     9\n",
            "..           ...      ...   ...\n",
            "409       327479    Diaby    10\n",
            "410       327936   Frauke     9\n",
            "410       327936    Petry    10\n",
            "411       331408   Frauke     9\n",
            "411       331408    Petry    10\n",
            "\n",
            "[826 rows x 3 columns]\n",
            "Labeled found entities in lemma data for class: EP_FINANZ\n",
            "     sentence_id     token label\n",
            "0            507  Emmanuel     5\n",
            "0            507    Macron     6\n",
            "1           1068  Emmanuel     5\n",
            "1           1068    Macron     6\n",
            "2           1089  Emmanuel     5\n",
            "..           ...       ...   ...\n",
            "197       299287   Nüßlein     6\n",
            "198       308752     Georg     5\n",
            "198       308752   Nüßlein     6\n",
            "199       321839     Georg     5\n",
            "199       321839   Nüßlein     6\n",
            "\n",
            "[400 rows x 3 columns]\n",
            "Labeled found entities in lemma data for class: EP_MEDIA\n",
            "     sentence_id      token label\n",
            "0            862  Christian     7\n",
            "0            862    Lindner     8\n",
            "1           3197       Imad     7\n",
            "1           3197      Karim     8\n",
            "2           8050  Christian     7\n",
            "..           ...        ...   ...\n",
            "272       333289    Lindner     8\n",
            "273       333725  Christian     7\n",
            "273       333725    Lindner     8\n",
            "274       333732  Christian     7\n",
            "274       333732    Lindner     8\n",
            "\n",
            "[551 rows x 3 columns]\n",
            "Labeled found entities in lemma data for class: EP_REL\n",
            "    sentence_id       token label\n",
            "0         14181        Ilse    11\n",
            "0         14181  Junkermann    12\n",
            "1         27503      Ludwig    11\n",
            "1         27503      Schick    12\n",
            "2         29076      Ludwig    11\n",
            "2         29076      Schick    12\n",
            "3         42550        Elke    11\n",
            "3         42550       König    12\n",
            "4         42608        Elke    11\n",
            "4         42608       König    12\n",
            "5         44959       Klaus    11\n",
            "5         44959      Müller    12\n",
            "6         52684  Constantin    11\n",
            "7         75636      Ludwig    11\n",
            "7         75636      Schick    12\n",
            "8         87805  Constantin    11\n",
            "9        263297       Anton    11\n",
            "9        263297    Losinger    12\n",
            "10       278562       Peter    11\n",
            "10       278562       Neher    12\n",
            "11       290010      Margot    11\n",
            "11       290010     Käßmann    12\n",
            "12       307810  Constantin    11\n",
            "13       314536  Constantin    11\n",
            "14       323344      Pierre    11\n",
            "14       323344       Vogel    12\n",
            "15       330449      Margot    11\n",
            "15       330449     Käßmann    12\n",
            "Labeled found entities in lemma data for class: EP_POL\n",
            "       sentence_id     token label\n",
            "0               16     Heike     1\n",
            "0               16  Baehrens     2\n",
            "1               16   Karamba     1\n",
            "1               16     Diaby     2\n",
            "2               16    Sabine     1\n",
            "...            ...       ...   ...\n",
            "11989       334233     Heidt     2\n",
            "11990       334248    Silvia     1\n",
            "11990       334248    Breher     2\n",
            "11991       334263      Ekin     1\n",
            "11991       334263   Deligöz     2\n",
            "\n",
            "[24460 rows x 3 columns]\n",
            "Labeled found entities in lemma data for class: EO_POL\n",
            "       sentence_id        token label\n",
            "0                9          mut    23\n",
            "1               25   Bundeswehr    23\n",
            "2               26   Bundeswehr    23\n",
            "3               27   Bundeswehr    23\n",
            "4               62  Deutschland    23\n",
            "...            ...          ...   ...\n",
            "40542       334135          mut    23\n",
            "40543       334149          mut    23\n",
            "40544       334176  Deutschland    23\n",
            "40545       334176  Deutschland    23\n",
            "40546       334184          mut    23\n",
            "\n",
            "[41129 rows x 3 columns]\n",
            "Labeled found entities in lemma data for class: EO_MIL\n",
            "     sentence_id          token label\n",
            "0            473         Marine    37\n",
            "1            477         Marine    37\n",
            "2            478         Marine    37\n",
            "3            486         Marine    37\n",
            "4            486         Marine    37\n",
            "..           ...            ...   ...\n",
            "489       330221       Kommando    37\n",
            "489       330221  Spezialkräfte    38\n",
            "490       330470      Luftwaffe    37\n",
            "491       331795       Kommando    37\n",
            "491       331795  Spezialkräfte    38\n",
            "\n",
            "[596 rows x 3 columns]\n",
            "Labeled found entities in lemma data for class: EO_WIRT\n",
            "      sentence_id token label\n",
            "0              49  ltur    25\n",
            "1              68  ltur    25\n",
            "2             113  ltur    25\n",
            "3             113  ltur    25\n",
            "4             219  ltur    25\n",
            "...           ...   ...   ...\n",
            "7571       333718  ltur    25\n",
            "7572       333963  ltur    25\n",
            "7573       334098   Sto    25\n",
            "7574       334098   Sto    25\n",
            "7575       334234  ltur    25\n",
            "\n",
            "[7687 rows x 3 columns]\n",
            "Labeled found entities in lemma data for class: P_NAT\n",
            "       sentence_id    token label\n",
            "0                1  Deutsch    43\n",
            "1                1  Deutsch    43\n",
            "2                2  Deutsch    43\n",
            "3                5  Deutsch    43\n",
            "4                5  Deutsch    43\n",
            "...            ...      ...   ...\n",
            "39015       334021  Deutsch    43\n",
            "39016       334039  Deutsch    43\n",
            "39017       334104  Deutsch    43\n",
            "39018       334176  Deutsch    43\n",
            "39019       334176  Deutsch    43\n",
            "\n",
            "[39020 rows x 3 columns]\n",
            "Labeled found entities in lemma data for class: EO_MEDIA\n",
            "       sentence_id    token label\n",
            "0               22     Welt    29\n",
            "1               44     Bild    29\n",
            "2              147  Phoenix    29\n",
            "3              209     Welt    29\n",
            "4              217     Welt    29\n",
            "...            ...      ...   ...\n",
            "19021       334234     Bild    29\n",
            "19022       334255     Bild    29\n",
            "19023       334269     Bild    29\n",
            "19024       334269     Bild    29\n",
            "19025       334269     Bild    29\n",
            "\n",
            "[19292 rows x 3 columns]\n",
            "Labeled found entities in lemma data for class: EO_KULT\n",
            "    sentence_id                    token label\n",
            "0         22482                 Stiftung    35\n",
            "0         22482              Preußischer    36\n",
            "0         22482             Kulturbesitz    36\n",
            "1         47603  Max-Planck-Gesellschaft    35\n",
            "2         51400  Max-Planck-Gesellschaft    35\n",
            "3         59978                 Stiftung    35\n",
            "3         59978              Preußischer    36\n",
            "3         59978             Kulturbesitz    36\n",
            "4         69338  Max-Planck-Gesellschaft    35\n",
            "5         94177  Max-Planck-Gesellschaft    35\n",
            "6         94332  Max-Planck-Gesellschaft    35\n",
            "7        107263                 Stiftung    35\n",
            "7        107263              Preußischer    36\n",
            "7        107263             Kulturbesitz    36\n",
            "8        148950  Max-Planck-Gesellschaft    35\n",
            "9        174236                 Stiftung    35\n",
            "9        174236              Preußischer    36\n",
            "9        174236             Kulturbesitz    36\n",
            "10       183960  Max-Planck-Gesellschaft    35\n",
            "11       187815              Cecilienhof    35\n",
            "12       187839                  Klassik    35\n",
            "12       187839                 Stiftung    36\n",
            "12       187839                   Weimar    36\n",
            "13       187864              Cecilienhof    35\n",
            "14       187868              Cecilienhof    35\n",
            "15       187869              Cecilienhof    35\n",
            "16       187919              Cecilienhof    35\n",
            "17       242401                 Stiftung    35\n",
            "17       242401              Preußischer    36\n",
            "17       242401             Kulturbesitz    36\n",
            "18       242810                 Stiftung    35\n",
            "18       242810              Preußischer    36\n",
            "18       242810             Kulturbesitz    36\n",
            "19       242863                 Stiftung    35\n",
            "19       242863              Preußischer    36\n",
            "19       242863             Kulturbesitz    36\n",
            "20       254407  Max-Planck-Gesellschaft    35\n",
            "21       258638  Max-Planck-Gesellschaft    35\n",
            "22       270692                 Stiftung    35\n",
            "22       270692              Preußischer    36\n",
            "22       270692             Kulturbesitz    36\n",
            "23       284507              Cecilienhof    35\n",
            "24       292009                 Stiftung    35\n",
            "24       292009              Preußischer    36\n",
            "24       292009             Kulturbesitz    36\n",
            "25       292078                 Stiftung    35\n",
            "25       292078              Preußischer    36\n",
            "25       292078             Kulturbesitz    36\n",
            "Labeled found entities in lemma data for class: EO_REL\n",
            "      sentence_id   token label\n",
            "0              13  Missio    33\n",
            "1             485  Missio    33\n",
            "2             485  Missio    33\n",
            "3             497  Missio    33\n",
            "4             511  Missio    33\n",
            "...           ...     ...   ...\n",
            "1473       328976  Missio    33\n",
            "1474       328978  Missio    33\n",
            "1475       329010  Missio    33\n",
            "1476       330452  Missio    33\n",
            "1477       332623  Missio    33\n",
            "\n",
            "[1487 rows x 3 columns]\n",
            "Labeled found entities in lemma data for class: EO_MOV\n",
            "      sentence_id           token label\n",
            "0             549  Menschenrechte    41\n",
            "1             665  Menschenrechte    41\n",
            "2             758  Menschenrechte    41\n",
            "3             781  Menschenrechte    41\n",
            "4             781    Kinderrechte    41\n",
            "...           ...             ...   ...\n",
            "3151       332727  Menschenrechte    41\n",
            "3152       332731  Menschenrechte    41\n",
            "3153       332745  Menschenrechte    41\n",
            "3154       332752  Menschenrechte    41\n",
            "3155       334085  Menschenrechte    41\n",
            "\n",
            "[3380 rows x 3 columns]\n",
            "Labeled found entities in lemma data for class: EO_NGO\n",
            "     sentence_id                token label\n",
            "0            550            Sea-Watch    39\n",
            "1           1412                   IG    39\n",
            "1           1412               Metall    40\n",
            "2           1558                   IG    39\n",
            "2           1558               Metall    40\n",
            "..           ...                  ...   ...\n",
            "311       325723       SOS-Kinderdorf    39\n",
            "312       327487  Verbraucherzentrale    39\n",
            "313       329223              Campact    39\n",
            "314       329224              Campact    39\n",
            "315       333196  Verbraucherzentrale    39\n",
            "\n",
            "[476 rows x 3 columns]\n",
            "Labeled found entities in lemma data for class: P_FUNC\n",
            "      sentence_id            token label\n",
            "0              23  Bundeskanzlerin    47\n",
            "1              75        Schneider    47\n",
            "2              86        Schneider    47\n",
            "3             148  Bundeskanzlerin    47\n",
            "4             149  Bundeskanzlerin    47\n",
            "...           ...              ...   ...\n",
            "6988       334078  Bundeskanzlerin    47\n",
            "6989       334120           Rhetor    47\n",
            "6990       334151     Freiwilliger    47\n",
            "6991       334246         Erzieher    47\n",
            "6992       334246         Erzieher    47\n",
            "\n",
            "[6993 rows x 3 columns]\n",
            "Labeled found entities in lemma data for class: GPE\n",
            "       sentence_id        token label\n",
            "0               62  Deutschland    55\n",
            "1               68  Deutschland    55\n",
            "2              101  Deutschland    55\n",
            "3              154      Jamaika    55\n",
            "4              158  Deutschland    55\n",
            "...            ...          ...   ...\n",
            "60000       334112       Türkei    55\n",
            "60001       334123     Lettland    55\n",
            "60002       334123      Litauen    55\n",
            "60003       334176  Deutschland    55\n",
            "60004       334176  Deutschland    55\n",
            "\n",
            "[60303 rows x 3 columns]\n",
            "Labeled found entities in lemma data for class: P_GEN\n",
            "       sentence_id   token label\n",
            "0               16    Volk    53\n",
            "1               21    Volk    53\n",
            "2               35    Volk    53\n",
            "3               35    Volk    53\n",
            "4               39    Volk    53\n",
            "...            ...     ...   ...\n",
            "24873       334265  Bürger    53\n",
            "24874       334266  Bürger    53\n",
            "24875       334268  Bürger    53\n",
            "24876       334268  Bürger    53\n",
            "24877       334269   Leute    53\n",
            "\n",
            "[24878 rows x 3 columns]\n",
            "Labeled found entities in lemma data for class: P_ETH\n",
            "        sentence_id token label\n",
            "0                 3   Han    45\n",
            "1                 7    Wa    45\n",
            "2                 7    Wa    45\n",
            "3                13    Ga    45\n",
            "4                16    Li    45\n",
            "...             ...   ...   ...\n",
            "103042       334270    Wa    45\n",
            "103043       334270    Ga    45\n",
            "103044       334270    Wa    45\n",
            "103045       334273    Li    45\n",
            "103046       334273    Ga    45\n",
            "\n",
            "[103047 rows x 3 columns]\n",
            "Labeled found entities in lemma data for class: EO_FINANZ\n",
            "    sentence_id         token label\n",
            "0          2875       Central    27\n",
            "1         10863  Hannoversche    27\n",
            "2         42793        Munich    27\n",
            "2         42793            Re    28\n",
            "3         42793        Munich    27\n",
            "3         42793            Re    28\n",
            "4         49565        Munich    27\n",
            "4         49565            Re    28\n",
            "5         49565        Munich    27\n",
            "5         49565            Re    28\n",
            "6         49567        Munich    27\n",
            "6         49567            Re    28\n",
            "7         49567        Munich    27\n",
            "7         49567            Re    28\n",
            "8         49569        Munich    27\n",
            "8         49569            Re    28\n",
            "9         55247       Colonia    27\n",
            "10        55255       Colonia    27\n",
            "11        55265       Colonia    27\n",
            "12        55416       Colonia    27\n",
            "13        83655       Colonia    27\n",
            "14        86229       Colonia    27\n",
            "15        88445        Munich    27\n",
            "15        88445            Re    28\n",
            "16       146560       Colonia    27\n",
            "17       172182       Central    27\n",
            "18       178692  Hannoversche    27\n",
            "19       196221       Central    27\n",
            "20       198196  Hannoversche    27\n",
            "21       202659  Hannoversche    27\n",
            "22       251146       Central    27\n",
            "23       251162       Central    27\n",
            "24       257183        Debeka    27\n",
            "25       311725        Debeka    27\n",
            "26       326098       Central    27\n",
            "Labeled found entities in lemma data for class: P_SOZ\n",
            "       sentence_id        token label\n",
            "0               36       Arbeit    51\n",
            "1               38        Macht    51\n",
            "2               51       Arbeit    51\n",
            "3               53        Macht    51\n",
            "4               77       Arbeit    51\n",
            "...            ...          ...   ...\n",
            "32904       334127  Geflüchtete    51\n",
            "32905       334130  Geflüchtete    51\n",
            "32906       334135       Arbeit    51\n",
            "32907       334188        Macht    51\n",
            "32908       334249       Arbeit    51\n",
            "\n",
            "[32916 rows x 3 columns]\n",
            "Labeled found entities in lemma data for class: P_AGE\n",
            "       sentence_id  token label\n",
            "0                5  Alter    49\n",
            "1                6  Alter    49\n",
            "2                7  Alter    49\n",
            "3               58  Alter    49\n",
            "4               62  Alter    49\n",
            "...            ...    ...   ...\n",
            "23138       334249   Kind    49\n",
            "23139       334249   Kind    49\n",
            "23140       334251   Kind    49\n",
            "23141       334259   Kind    49\n",
            "23142       334269   Kind    49\n",
            "\n",
            "[23143 rows x 3 columns]\n",
            "Labeled found entities in lemma data for class: EP_OWN\n",
            "         sentence_id token label\n",
            "0                  0   ich    21\n",
            "1                  1  Mein    21\n",
            "2                  1   Ich    21\n",
            "3                  1   ich    21\n",
            "4                  1   ich    21\n",
            "...              ...   ...   ...\n",
            "1775427       334273   ich    21\n",
            "1775428       334273   ich    21\n",
            "1775429       334273   ich    21\n",
            "1775430       334273  mein    21\n",
            "1775431       334274   ich    21\n",
            "\n",
            "[1775432 rows x 3 columns]\n"
          ]
        }
      ],
      "source": [
        "class_mapping = {0: 'O',\n",
        " 1: 'B-EP_POL',\n",
        " 2: 'I-EP_POL',\n",
        " 3: 'B-EP_WIRT',\n",
        " 4: 'I-EP_WIRT',\n",
        " 5: 'B-EP_FINANZ',\n",
        " 6: 'I-EP_FINANZ',\n",
        " 7: 'B-EP_MEDIA',\n",
        " 8: 'I-EP_MEDIA',\n",
        " 9: 'B-EP_SCI',\n",
        " 10: 'I-EP_SCI',\n",
        " 11: 'B-EP_REL',\n",
        " 12: 'I-EP_REL',\n",
        " 13: 'B-EP_KULT',\n",
        " 14: 'I-EP_KULT',\n",
        " 15: 'B-EP_MIL',\n",
        " 16: 'I-EP_MIL',\n",
        " 17: 'B-EP_NGO',\n",
        " 18: 'I-EP_NGO',\n",
        " 19: 'B-EP_MOV',\n",
        " 20: 'I-EP_MOV',\n",
        " 21: 'B-EP_OWN',\n",
        " 22: 'I-EP_OWN',\n",
        " 23: 'B-EO_POL',\n",
        " 24: 'I-EO_POL',\n",
        " 25: 'B-EO_WIRT',\n",
        " 26: 'I-EO_WIRT',\n",
        " 27: 'B-EO_FINANZ',\n",
        " 28: 'I-EO_FINANZ',\n",
        " 29: 'B-EO_MEDIA',\n",
        " 30: 'I-EO_MEDIA',\n",
        " 31: 'B-EO_SCI',\n",
        " 32: 'I-EO_SCI',\n",
        " 33: 'B-EO_REL',\n",
        " 34: 'I-EO_REL',\n",
        " 35: 'B-EO_KULT',\n",
        " 36: 'I-EO_KULT',\n",
        " 37: 'B-EO_MIL',\n",
        " 38: 'I-EO_MIL',\n",
        " 39: 'B-EO_NGO',\n",
        " 40: 'I-EO_NGO',\n",
        " 41: 'B-EO_MOV',\n",
        " 42: 'I-EO_MOV',\n",
        " 43: 'B-P_NAT',\n",
        " 44: 'I-P_NAT',\n",
        " 45: 'B-P_ETH',\n",
        " 46: 'I-P_ETH',\n",
        " 47: 'B-P_FUNC',\n",
        " 48: 'I-P_FUNC',\n",
        " 49: 'B-P_AGE',\n",
        " 50: 'I-P_AGE',\n",
        " 51: 'B-P_SOZ',\n",
        " 52: 'I-P_SOZ',\n",
        " 53: 'B-P_GEN',\n",
        " 54: 'I-P_GEN',\n",
        " 55: 'B-GPE',\n",
        " 56: 'I-GPE'}\n",
        "\n",
        "print('++++++++++ Label Found Entities ... ++++++++++')\n",
        "\n",
        "\n",
        "for c, df in extracted_lists.items():\n",
        "  key = list(class_mapping.keys())[list(class_mapping.values()).index(\"B-\"+c)]\n",
        "\n",
        "  # label found entities from original data\n",
        "  res = label(df, key)\n",
        "\n",
        "  # save labeled/found entities\n",
        "  res.to_csv(path_found + str(c) + '_extracted_normal_labels.csv', index=False)\n",
        "\n",
        "  # for further usage -> remove incorrect labels based on heuristic\n",
        "  extracted_lists[c] = correct_labels(res,c)\n",
        "\n",
        "  print('Labeled found entities in normal data for class: ' + str(c) + ' and saved in directory: ' + path_found)\n",
        " \n",
        "  \n",
        "\n",
        "for c, df in extracted_lists_lem.items():\n",
        "  key = list(class_mapping.keys())[list(class_mapping.values()).index(\"B-\"+c)]\n",
        "\n",
        "  # label found entities from lemmatized data\n",
        "  res = label(df, key)\n",
        "\n",
        "  # save labeled/found entities\n",
        "  res.to_csv(path_found + str(c) + '_extracted_lemma_labels.csv', index=False)\n",
        "\n",
        "  # for further usage -> remove incorrect labels based on heuristic\n",
        "  extracted_lists_lem[c] = correct_labels(res,c)\n",
        "  \n",
        "  print('Labeled found entities in lemmatized data for class: ' + str(c) + ' and saved in directory: ' + path_found)\n",
        " \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SzPMi4Nr41bg"
      },
      "source": [
        "# 07 B - Loading Found & Labeled Instances\n",
        "- for further usage: clean entities based on heuristics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xt36FHg148iW",
        "outputId": "d8197b65-ecfd-4cae-8bc8-ea2bc8812f0e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "++++++++++ Reading in found entity samples with labels ... ++++++++++\n",
            "Removed 0 ambiguous/incorrect tokens for class EO_SCI\n",
            "Removed 0 ambiguous/incorrect tokens for class EP_MEDIA\n",
            "Removed 0 ambiguous/incorrect tokens for class EP_WIRT\n",
            "Removed 0 ambiguous/incorrect tokens for class EP_REL\n",
            "Removed 0 ambiguous/incorrect tokens for class EP_POL\n",
            "Removed 0 ambiguous/incorrect tokens for class EP_FINANZ\n",
            "Removed 0 ambiguous/incorrect tokens for class EP_SCI\n",
            "Removed 0 ambiguous/incorrect tokens for class EP_NGO\n",
            "Removed 0 ambiguous/incorrect tokens for class EP_KULT\n",
            "Removed 0 ambiguous/incorrect tokens for class EP_MOV\n",
            "Removed 0 ambiguous/incorrect tokens for class EP_MIL\n",
            "Removed 34568 ambiguous/incorrect tokens for class EO_POL\n",
            "Removed 0 ambiguous/incorrect tokens for class EO_MIL\n",
            "Removed 0 ambiguous/incorrect tokens for class EO_REL\n",
            "Removed 9553 ambiguous/incorrect tokens for class EO_MEDIA\n",
            "Removed 0 ambiguous/incorrect tokens for class EO_KULT\n",
            "Removed 4277 ambiguous/incorrect tokens for class P_FUNC\n",
            "Removed 0 ambiguous/incorrect tokens for class EO_WIRT\n",
            "Removed 57 ambiguous/incorrect tokens for class EO_MOV\n",
            "Removed 0 ambiguous/incorrect tokens for class EO_NGO\n",
            "Removed 890 ambiguous/incorrect tokens for class P_NAT\n",
            "Removed 0 ambiguous/incorrect tokens for class GPE\n",
            "Removed 0 ambiguous/incorrect tokens for class P_GEN\n",
            "Removed 0 ambiguous/incorrect tokens for class EO_FINANZ\n",
            "Removed 193 ambiguous/incorrect tokens for class P_ETH\n",
            "Removed 31302 ambiguous/incorrect tokens for class P_SOZ\n",
            "Removed 0 ambiguous/incorrect tokens for class P_AGE\n",
            "Removed 0 ambiguous/incorrect tokens for class EP_OWN\n",
            "Read in 28 entity lists with labels from normal data\n",
            "Removed 0 ambiguous/incorrect tokens for class EP_WIRT\n",
            "Removed 0 ambiguous/incorrect tokens for class EO_SCI\n",
            "Removed 0 ambiguous/incorrect tokens for class EP_KULT\n",
            "Removed 0 ambiguous/incorrect tokens for class EP_MOV\n",
            "Removed 0 ambiguous/incorrect tokens for class EP_SCI\n",
            "Removed 0 ambiguous/incorrect tokens for class EP_MIL\n",
            "Removed 0 ambiguous/incorrect tokens for class EP_MEDIA\n",
            "Removed 0 ambiguous/incorrect tokens for class EP_FINANZ\n",
            "Removed 0 ambiguous/incorrect tokens for class EP_NGO\n",
            "Removed 0 ambiguous/incorrect tokens for class EP_REL\n",
            "Removed 0 ambiguous/incorrect tokens for class EP_POL\n",
            "Removed 0 ambiguous/incorrect tokens for class EO_MIL\n",
            "Removed 57 ambiguous/incorrect tokens for class EO_MOV\n",
            "Removed 0 ambiguous/incorrect tokens for class EO_WIRT\n",
            "Removed 0 ambiguous/incorrect tokens for class EO_KULT\n",
            "Removed 33511 ambiguous/incorrect tokens for class EO_POL\n",
            "Removed 9549 ambiguous/incorrect tokens for class EO_MEDIA\n",
            "Removed 0 ambiguous/incorrect tokens for class EO_REL\n",
            "Removed 890 ambiguous/incorrect tokens for class P_NAT\n",
            "Removed 0 ambiguous/incorrect tokens for class EO_NGO\n",
            "Removed 4277 ambiguous/incorrect tokens for class P_FUNC\n",
            "Removed 31404 ambiguous/incorrect tokens for class P_SOZ\n",
            "Removed 0 ambiguous/incorrect tokens for class P_AGE\n",
            "Removed 0 ambiguous/incorrect tokens for class P_GEN\n",
            "Removed 0 ambiguous/incorrect tokens for class EO_FINANZ\n",
            "Removed 4 ambiguous/incorrect tokens for class P_ETH\n",
            "Removed 0 ambiguous/incorrect tokens for class GPE\n",
            "Removed 0 ambiguous/incorrect tokens for class EP_OWN\n",
            "Read in 28 entity lists with labels from lemma data\n"
          ]
        }
      ],
      "source": [
        "print('++++++++++ Loading Found Entity Samples with Labels ... ++++++++++')\n",
        "\n",
        "all_found_entities_files = glob.glob(os.path.join(path_found, '*_extracted_normal_labels.csv'))\n",
        "all_found_entities_lemmas_files = glob.glob(os.path.join(path_found, '*_extracted_lemma_labels.csv'))\n",
        "\n",
        "extracted_lists = {}\n",
        "extracted_lists_lem = {}\n",
        "\n",
        "for f in all_found_entities_files:\n",
        "  df = pd.read_csv(f)\n",
        "  c = f.replace('_extracted_normal_labels.csv', '').replace(path_found, '')\n",
        "  extracted_lists[c] = correct_labels(df, c)\n",
        "  \n",
        "print('Read in ' + str(len(extracted_lists)) + ' entity lists with labels from normal data') \n",
        "\n",
        "for f in all_found_entities_lemmas_files:\n",
        "  df = pd.read_csv(f)\n",
        "  c = f.replace('_extracted_lemma_labels.csv', '').replace(path_found, '')\n",
        "  extracted_lists_lem[c] = correct_labels(df, c)\n",
        "print('Read in ' + str(len(extracted_lists_lem)) + ' entity lists with labels from lemma data') \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZ-SFwN531Vo"
      },
      "source": [
        "# 08 - Annotating Training Data + Separating Data for Lexicon Generation\n",
        "- Use found & labeled entities to annotate data\n",
        "- Separating data for lexicon generation by removing this from the data dedicated to fine-tuning the language model\n",
        "- Save training data and lexicon data to files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QoInWG22eH65",
        "outputId": "58510534-1461-4e2c-f367-3ad853d97b9a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "++++++++++ Annotating training data ... ++++++++++\n",
            "Merging normal data with class 1: EO_SCI\n",
            "Merging normal data with class 2: EP_MEDIA\n",
            "Merging normal data with class 3: EP_WIRT\n",
            "Merging normal data with class 4: EP_REL\n",
            "Merging normal data with class 5: EP_POL\n",
            "Merging normal data with class 6: EP_FINANZ\n",
            "Merging normal data with class 7: EP_SCI\n",
            "Merging normal data with class 8: EP_NGO\n",
            "Merging normal data with class 9: EP_KULT\n",
            "Merging normal data with class 10: EP_MOV\n",
            "Merging normal data with class 11: EP_MIL\n",
            "Merging normal data with class 12: EO_POL\n",
            "Merging normal data with class 13: EO_MIL\n",
            "Merging normal data with class 14: EO_REL\n",
            "Merging normal data with class 15: EO_MEDIA\n",
            "Merging normal data with class 16: EO_KULT\n",
            "Merging normal data with class 17: P_FUNC\n",
            "Merging normal data with class 18: EO_WIRT\n",
            "Merging normal data with class 19: EO_MOV\n",
            "Merging normal data with class 20: EO_NGO\n",
            "Merging normal data with class 21: P_NAT\n",
            "Merging normal data with class 22: GPE\n",
            "Merging normal data with class 23: P_GEN\n",
            "Merging normal data with class 24: EO_FINANZ\n",
            "Merging normal data with class 25: P_ETH\n",
            "Merging normal data with class 26: P_SOZ\n",
            "Merging normal data with class 27: P_AGE\n",
            "Merging normal data with class 28: EP_OWN\n",
            "Merging lemma data with class 1: EP_WIRT\n",
            "Merging lemma data with class 2: EO_SCI\n",
            "Merging lemma data with class 3: EP_KULT\n",
            "Merging lemma data with class 4: EP_MOV\n",
            "Merging lemma data with class 5: EP_SCI\n",
            "Merging lemma data with class 6: EP_MIL\n",
            "Merging lemma data with class 7: EP_MEDIA\n",
            "Merging lemma data with class 8: EP_FINANZ\n",
            "Merging lemma data with class 9: EP_NGO\n",
            "Merging lemma data with class 10: EP_REL\n",
            "Merging lemma data with class 11: EP_POL\n",
            "Merging lemma data with class 12: EO_MIL\n",
            "Merging lemma data with class 13: EO_MOV\n",
            "Merging lemma data with class 14: EO_WIRT\n",
            "Merging lemma data with class 15: EO_KULT\n",
            "Merging lemma data with class 16: EO_POL\n",
            "Merging lemma data with class 17: EO_MEDIA\n",
            "Merging lemma data with class 18: EO_REL\n",
            "Merging lemma data with class 19: P_NAT\n",
            "Merging lemma data with class 20: EO_NGO\n",
            "Merging lemma data with class 21: P_FUNC\n",
            "Merging lemma data with class 22: P_SOZ\n",
            "Merging lemma data with class 23: P_AGE\n",
            "Merging lemma data with class 24: P_GEN\n",
            "Merging lemma data with class 25: EO_FINANZ\n",
            "Merging lemma data with class 26: P_ETH\n",
            "Merging lemma data with class 27: GPE\n",
            "Merging lemma data with class 28: EP_OWN\n",
            "8229930\n",
            "8229929\n",
            "Annotated training data based on extracted entities from normal and lemmatized data.\n",
            "Saved training data in directory: ./drive/MyDrive/model/data/training/training_data/\n",
            "Duration: 0:09:51.311023\n"
          ]
        }
      ],
      "source": [
        "# estimated duration: 10 min\n",
        "\n",
        "print('++++++++++ Annotating training data ... ++++++++++')\n",
        "i = 1\n",
        "\n",
        "start = timer()\n",
        "\n",
        "for c, normal in extracted_lists.items():\n",
        "    print('Merging normal data with class ' + str(i) + ': ' + (str(c)))\n",
        "\n",
        "    # subsequently merge original data with labeled/extracted entities based on sentence id and token to annotate data\n",
        "    data = merge_text_label(data, normal, ['sentence_id', 'token'])\n",
        "    i+=1\n",
        "\n",
        "i = 1\n",
        "for c, lemma in extracted_lists_lem.items():\n",
        "    print('Merging lemma data with class ' + str(i) + ': ' + (str(c)))\n",
        "    # subsequently merge lemmatized data with labeled/extracted entities based on sentence id and token to annotate data\n",
        "    data_lem = merge_text_label(data_lem, lemma, ['sentence_id', 'token'])\n",
        "    i+=1\n",
        "\n",
        "# merge original and lemmatized data\n",
        "data['label'] = data['label'].fillna(data_lem['label'])\n",
        "\n",
        "# create O-labels\n",
        "data['label'] = data['label'].fillna(0)\n",
        "print('Annotated data based on extracted entities from normal and lemmatized data.')\n",
        "\n",
        "# separate data used for lexicon generation\n",
        "lex_data = data.iloc[int(len(data)/2):]\n",
        "data = data.iloc[:int(len(data)/2)]\n",
        "\n",
        "print(len(lex_data))\n",
        "print(len(data))\n",
        "\n",
        "print('Partitioned data for lexicon generation.')\n",
        "lex_data.to_csv(path_lexicon + 'lex_data.csv', index=False)\n",
        "print('Saved data for lexicon generation in directory: ' + path_lexicon)\n",
        "\n",
        "data.to_csv(path_training_data + 'training_data.csv', index=False)\n",
        "print('Saved training data in directory: ' + path_training_data)\n",
        "\n",
        "end = timer()\n",
        "\n",
        "print('Total Duration: ' + str(timedelta(seconds=end - start)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKeXjGCs6CeX"
      },
      "source": [
        "# 08 B - Load Annotated Training Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "quKyYcly5-Ah"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv(path_training_data + 'training_data.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PfjGs2GwACyx"
      },
      "source": [
        "# 09 - Count Labeled Entities in Training Data\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TO_UrODpP25n",
        "outputId": "c5817da7-016e-4847-b4c3-bf00562d911b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "         sentence_id    token  label\n",
            "9                  0      Sie   21.0\n",
            "25                 1      Ich   21.0\n",
            "27                 1      Sie   21.0\n",
            "37                 1       Es   21.0\n",
            "129                3      ich   21.0\n",
            "...              ...      ...    ...\n",
            "8229859       167658       es   21.0\n",
            "8229865       167659       Er   21.0\n",
            "8229893       167659  unserer   21.0\n",
            "8229898       167659  unserer   21.0\n",
            "8229925       167660      wir   21.0\n",
            "\n",
            "[576124 rows x 3 columns]\n",
            "token             label\n",
            "wir               21.0     89380\n",
            "Sie               21.0     69608\n",
            "es                21.0     61603\n",
            "Wir               21.0     39664\n",
            "ich               21.0     37439\n",
            "                           ...  \n",
            "Tschechoslowakei  55.0         1\n",
            "Müllner           16.0         1\n",
            "Dichtern          47.0         1\n",
            "NS-Staat          55.0         1\n",
            "Grenadiers        43.0         1\n",
            "Length: 1632, dtype: int64\n",
            "                            0\n",
            "token            label       \n",
            "wir              21.0   89380\n",
            "Sie              21.0   69608\n",
            "es               21.0   61603\n",
            "Wir              21.0   39664\n",
            "ich              21.0   37439\n",
            "...                       ...\n",
            "Tschechoslowakei 55.0       1\n",
            "Müllner          16.0       1\n",
            "Dichtern         47.0       1\n",
            "NS-Staat         55.0       1\n",
            "Grenadiers       43.0       1\n",
            "\n",
            "[1632 rows x 1 columns]\n"
          ]
        }
      ],
      "source": [
        "label_count = count_found_entities(data)\n",
        "label_count.to_csv(path_training_data + 'label_count_training_data.csv', index=False)\n",
        "label_count"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSf5W17W3-NE"
      },
      "source": [
        "# 10 A - Merging Annotated Training Data Back to Sentences\n",
        "- The unlabeled data was previously split into tokens. Now the (annotated) training data is restructured into paragraphs for the fine-tuning of the BERT model\n",
        "- The restructed data is then saved to file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 572
        },
        "id": "AFjPkqvDjUD5",
        "outputId": "728b85af-7994-430c-e4fa-1742b6627d60"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Built 0 sentences.\n",
            "Built 20000 sentences.\n",
            "Built 40000 sentences.\n",
            "Built 60000 sentences.\n",
            "Built 80000 sentences.\n",
            "Built 100000 sentences.\n",
            "Built 120000 sentences.\n",
            "Built 140000 sentences.\n",
            "Built 160000 sentences.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-5cd54880-3565-4ba4-be7b-45b517ce5b91\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence_id</th>\n",
              "      <th>token</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>[Guten, Morgen, ,, liebe, Kolleginnen, und, Ko...</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>[Meine, sehr, verehrten, Damen, und, Herren, !...</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>[§, 1, Absatz, 2, der, Geschäftsordnung, des, ...</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>[Die, Fraktion, der, AfD, widerspricht, diesem...</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>[Enthaltungen, ?, –, Der, Antrag, ist, damit, ...</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>167656</th>\n",
              "      <td>167656</td>\n",
              "      <td>[Aber, Ihre, Vorschläge, sind, weder, eine, Al...</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>167657</th>\n",
              "      <td>167657</td>\n",
              "      <td>[Bevor, ich, zu, den, Herausforderungen, komme...</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>167658</th>\n",
              "      <td>167658</td>\n",
              "      <td>[Auch, wenn, bei, diesem, Fall, viel, schiefge...</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>167659</th>\n",
              "      <td>167659</td>\n",
              "      <td>[Und, :, Er, ist, in, Haft, ,, liebe, Kollegin...</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>167660</th>\n",
              "      <td>167660</td>\n",
              "      <td>[Natürlich, haben, wir, noch, Themen, ,]</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>167661 rows × 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5cd54880-3565-4ba4-be7b-45b517ce5b91')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-5cd54880-3565-4ba4-be7b-45b517ce5b91 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-5cd54880-3565-4ba4-be7b-45b517ce5b91');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "        sentence_id                                              token  \\\n",
              "0                 0  [Guten, Morgen, ,, liebe, Kolleginnen, und, Ko...   \n",
              "1                 1  [Meine, sehr, verehrten, Damen, und, Herren, !...   \n",
              "2                 2  [§, 1, Absatz, 2, der, Geschäftsordnung, des, ...   \n",
              "3                 3  [Die, Fraktion, der, AfD, widerspricht, diesem...   \n",
              "4                 4  [Enthaltungen, ?, –, Der, Antrag, ist, damit, ...   \n",
              "...             ...                                                ...   \n",
              "167656       167656  [Aber, Ihre, Vorschläge, sind, weder, eine, Al...   \n",
              "167657       167657  [Bevor, ich, zu, den, Herausforderungen, komme...   \n",
              "167658       167658  [Auch, wenn, bei, diesem, Fall, viel, schiefge...   \n",
              "167659       167659  [Und, :, Er, ist, in, Haft, ,, liebe, Kollegin...   \n",
              "167660       167660           [Natürlich, haben, wir, noch, Themen, ,]   \n",
              "\n",
              "                                                    label  \n",
              "0       [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
              "1       [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
              "2       [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
              "3       [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
              "4       [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
              "...                                                   ...  \n",
              "167656  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
              "167657  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
              "167658  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
              "167659  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
              "167660                     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]  \n",
              "\n",
              "[167661 rows x 3 columns]"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "t = []\n",
        "l = []\n",
        "s_ids = []\n",
        "\n",
        "# group paragraphs\n",
        "paragraphs = data.groupby(['sentence_id'])\n",
        "\n",
        "for name,group in paragraphs:\n",
        "  t.append(group.token.values.tolist())\n",
        "  l.append(group.label.values.tolist())\n",
        "  s_ids.append(name)\n",
        "  if name % 20000 == 0:\n",
        "    print('Built ' + str(name) + ' sentences.')\n",
        "data = pd.DataFrame({'sentence_id' : s_ids, 'token' : t, 'label' : l })\n",
        "\n",
        "# save training data in sentences\n",
        "data.to_csv(path_training_data + 'training_data_sentences.csv', index=False)\n",
        "data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FrEu__HV8VDF"
      },
      "source": [
        "# 10 B - Loading Training Data as Sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "67b-Za-5rWPi",
        "outputId": "e1dbf605-c2e3-420e-8efb-5300c0b8a57d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-3fb34926-3020-4d90-87ac-dd627f00b696\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence_id</th>\n",
              "      <th>token</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>['Guten', 'Morgen', ',', 'liebe', 'Kolleginnen...</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>['Meine', 'sehr', 'verehrten', 'Damen', 'und',...</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>['§', '1', 'Absatz', '2', 'der', 'Geschäftsord...</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>['Die', 'Fraktion', 'der', 'AfD', 'widersprich...</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>['Enthaltungen', '?', '–', 'Der', 'Antrag', 'i...</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>167656</th>\n",
              "      <td>167656</td>\n",
              "      <td>['Aber', 'Ihre', 'Vorschläge', 'sind', 'weder'...</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>167657</th>\n",
              "      <td>167657</td>\n",
              "      <td>['Bevor', 'ich', 'zu', 'den', 'Herausforderung...</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>167658</th>\n",
              "      <td>167658</td>\n",
              "      <td>['Auch', 'wenn', 'bei', 'diesem', 'Fall', 'vie...</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>167659</th>\n",
              "      <td>167659</td>\n",
              "      <td>['Und', ':', 'Er', 'ist', 'in', 'Haft', ',', '...</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>167660</th>\n",
              "      <td>167660</td>\n",
              "      <td>['Natürlich', 'haben', 'wir', 'noch', 'Themen'...</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>167661 rows × 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3fb34926-3020-4d90-87ac-dd627f00b696')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3fb34926-3020-4d90-87ac-dd627f00b696 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3fb34926-3020-4d90-87ac-dd627f00b696');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "        sentence_id                                              token  \\\n",
              "0                 0  ['Guten', 'Morgen', ',', 'liebe', 'Kolleginnen...   \n",
              "1                 1  ['Meine', 'sehr', 'verehrten', 'Damen', 'und',...   \n",
              "2                 2  ['§', '1', 'Absatz', '2', 'der', 'Geschäftsord...   \n",
              "3                 3  ['Die', 'Fraktion', 'der', 'AfD', 'widersprich...   \n",
              "4                 4  ['Enthaltungen', '?', '–', 'Der', 'Antrag', 'i...   \n",
              "...             ...                                                ...   \n",
              "167656       167656  ['Aber', 'Ihre', 'Vorschläge', 'sind', 'weder'...   \n",
              "167657       167657  ['Bevor', 'ich', 'zu', 'den', 'Herausforderung...   \n",
              "167658       167658  ['Auch', 'wenn', 'bei', 'diesem', 'Fall', 'vie...   \n",
              "167659       167659  ['Und', ':', 'Er', 'ist', 'in', 'Haft', ',', '...   \n",
              "167660       167660  ['Natürlich', 'haben', 'wir', 'noch', 'Themen'...   \n",
              "\n",
              "                                                    label  \n",
              "0       [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
              "1       [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
              "2       [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
              "3       [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
              "4       [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
              "...                                                   ...  \n",
              "167656  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
              "167657  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
              "167658  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
              "167659  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
              "167660                     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]  \n",
              "\n",
              "[167661 rows x 3 columns]"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data = pd.read_csv(path_training_data + 'training_data_sentences.csv')\n",
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "row_aVnj9G5Z"
      },
      "source": [
        "# 11 - Remove Incorrect Inner Labels\n",
        "- The annotation caused incorrect inner labels within the training data. These are removed. \n",
        "- The corrected training data is saved to a file, replacing the previous version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451,
          "referenced_widgets": [
            "391a35e235bf47fc843ecdaefe54edf2",
            "4a0b509ccbaf4068b42ca6e437cc59c3",
            "f84f041bf88b40829fd261332ea3ff99",
            "0c340acd35944feb94312fdb41392381",
            "c603036b0a934c3393129811315362b8",
            "0cb35045cf51412a8b81318a89be3d0d",
            "4eaf2cea82de4332955ac1ceb544c19c",
            "cc0d0c10fab04baa9c346d1eb6e7c1a3",
            "b5399d6e2d13422497aa4a3f0ea81762",
            "4b5eb580db624d1a8d8ef82599877abb",
            "e83d8e0b16364d8b8b8c3975c33f1c43"
          ]
        },
        "id": "wpFo4otRn0lk",
        "outputId": "03363fa3-31c8-4104-fbf2-c4782472c2a8"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "391a35e235bf47fc843ecdaefe54edf2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "progress:   0%|          | 0/167661 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-e04f9633-512e-4ad4-ad47-d537b71601c4\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence_id</th>\n",
              "      <th>token</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>['Guten', 'Morgen', ',', 'liebe', 'Kolleginnen...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>['Meine', 'sehr', 'verehrten', 'Damen', 'und',...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>['§', '1', 'Absatz', '2', 'der', 'Geschäftsord...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>['Die', 'Fraktion', 'der', 'AfD', 'widersprich...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>['Enthaltungen', '?', '–', 'Der', 'Antrag', 'i...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>167656</th>\n",
              "      <td>167656</td>\n",
              "      <td>['Aber', 'Ihre', 'Vorschläge', 'sind', 'weder'...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>167657</th>\n",
              "      <td>167657</td>\n",
              "      <td>['Bevor', 'ich', 'zu', 'den', 'Herausforderung...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>167658</th>\n",
              "      <td>167658</td>\n",
              "      <td>['Auch', 'wenn', 'bei', 'diesem', 'Fall', 'vie...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>167659</th>\n",
              "      <td>167659</td>\n",
              "      <td>['Und', ':', 'Er', 'ist', 'in', 'Haft', ',', '...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>167660</th>\n",
              "      <td>167660</td>\n",
              "      <td>['Natürlich', 'haben', 'wir', 'noch', 'Themen'...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>167661 rows × 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e04f9633-512e-4ad4-ad47-d537b71601c4')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e04f9633-512e-4ad4-ad47-d537b71601c4 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e04f9633-512e-4ad4-ad47-d537b71601c4');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "        sentence_id                                              token  \\\n",
              "0                 0  ['Guten', 'Morgen', ',', 'liebe', 'Kolleginnen...   \n",
              "1                 1  ['Meine', 'sehr', 'verehrten', 'Damen', 'und',...   \n",
              "2                 2  ['§', '1', 'Absatz', '2', 'der', 'Geschäftsord...   \n",
              "3                 3  ['Die', 'Fraktion', 'der', 'AfD', 'widersprich...   \n",
              "4                 4  ['Enthaltungen', '?', '–', 'Der', 'Antrag', 'i...   \n",
              "...             ...                                                ...   \n",
              "167656       167656  ['Aber', 'Ihre', 'Vorschläge', 'sind', 'weder'...   \n",
              "167657       167657  ['Bevor', 'ich', 'zu', 'den', 'Herausforderung...   \n",
              "167658       167658  ['Auch', 'wenn', 'bei', 'diesem', 'Fall', 'vie...   \n",
              "167659       167659  ['Und', ':', 'Er', 'ist', 'in', 'Haft', ',', '...   \n",
              "167660       167660  ['Natürlich', 'haben', 'wir', 'noch', 'Themen'...   \n",
              "\n",
              "                                                    label  \n",
              "0                 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
              "1       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
              "2       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
              "3       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
              "4       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
              "...                                                   ...  \n",
              "167656            [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
              "167657  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
              "167658  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
              "167659  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
              "167660                                 [0, 0, 0, 0, 0, 0]  \n",
              "\n",
              "[167661 rows x 3 columns]"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tqdm.pandas(desc='Removing Faulty Inner Labels')\n",
        "data['label'] = data['label'].progress_apply(remove_inner_labels)\n",
        "data.to_csv(path_training_data + 'training_data_sentences.csv', index=False)\n",
        "data\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "3g5G-19NljIa",
        "Itd1DRM2lvSu",
        "OI4xckcTZay0",
        "Bu5-d3PhlTrf",
        "PYWQE4fUpNgh",
        "4aPyHOpnlz8K",
        "_bxJPNAyl78b",
        "FRlGzprn5lbw",
        "A2CQEysTm2Wf",
        "6jxm13tvhnTH",
        "SzPMi4Nr41bg",
        "TZ-SFwN531Vo",
        "aKeXjGCs6CeX",
        "PfjGs2GwACyx",
        "sSf5W17W3-NE",
        "FrEu__HV8VDF",
        "row_aVnj9G5Z"
      ],
      "name": "NEW_Pipeline.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
    },
    "kernelspec": {
      "display_name": "Python 3.9.12 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0c340acd35944feb94312fdb41392381": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4b5eb580db624d1a8d8ef82599877abb",
            "placeholder": "​",
            "style": "IPY_MODEL_e83d8e0b16364d8b8b8c3975c33f1c43",
            "value": " 167661/167661 [00:05&lt;00:00, 32558.54it/s]"
          }
        },
        "0cb35045cf51412a8b81318a89be3d0d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "14bec54aa19d4082bc432c8fc2a2b2cb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2a392235e69442c29da94f08d54bf37d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3e70c976c0bf439c8f126ea4d704bbb5",
            "placeholder": "​",
            "style": "IPY_MODEL_a8f0291a60c34fa0a110fa9c162a869b",
            "value": " 334276/334276 [03:34&lt;00:00, 1774.55it/s]"
          }
        },
        "33177bf842ee4ff98da93440a6eb78d0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "391a35e235bf47fc843ecdaefe54edf2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4a0b509ccbaf4068b42ca6e437cc59c3",
              "IPY_MODEL_f84f041bf88b40829fd261332ea3ff99",
              "IPY_MODEL_0c340acd35944feb94312fdb41392381"
            ],
            "layout": "IPY_MODEL_c603036b0a934c3393129811315362b8"
          }
        },
        "3e70c976c0bf439c8f126ea4d704bbb5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3ea78bff89b142dbb454a476bda51ae9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4a0b509ccbaf4068b42ca6e437cc59c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0cb35045cf51412a8b81318a89be3d0d",
            "placeholder": "​",
            "style": "IPY_MODEL_4eaf2cea82de4332955ac1ceb544c19c",
            "value": "progress: 100%"
          }
        },
        "4b5eb580db624d1a8d8ef82599877abb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4eaf2cea82de4332955ac1ceb544c19c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "51dba03ba8c8495ca675a9f74af5e226": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b2f0489be184f2696c9744530ad68b5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5c87632296624fc78b71f8a82e1b2f7d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6a6bb2605c3d4faf978fd318d6e4e55e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8f24188cfba347b3a01f2385456e6d21",
              "IPY_MODEL_d5cf9cc6b5f340e392b434671c024c0e",
              "IPY_MODEL_2a392235e69442c29da94f08d54bf37d"
            ],
            "layout": "IPY_MODEL_14bec54aa19d4082bc432c8fc2a2b2cb"
          }
        },
        "73958a2143e64052b16908e7af530ca4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b2037c837d94d1a8734e7d8ac458cf9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8474896ba19c49d886883c8561a50796": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "89ffc130f5b14dc8b474150f2c34970c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8f24188cfba347b3a01f2385456e6d21": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c44305f42df3435f9145c6dc30adbe07",
            "placeholder": "​",
            "style": "IPY_MODEL_89ffc130f5b14dc8b474150f2c34970c",
            "value": "Lemmatizing data: 100%"
          }
        },
        "9c487735b9c84b8e9707e442552e3476": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_51dba03ba8c8495ca675a9f74af5e226",
            "placeholder": "​",
            "style": "IPY_MODEL_fb84f72a0cea42b083cccdca84c6654b",
            "value": " 334276/334276 [02:35&lt;00:00, 2511.22it/s]"
          }
        },
        "a8f0291a60c34fa0a110fa9c162a869b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b5399d6e2d13422497aa4a3f0ea81762": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c3ab738d1513478cb0963b406624831d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ea3d49ff563442fb8919b475bb05cbf0",
              "IPY_MODEL_f949f184b5d04853a86d3fdcbb3ca7fe",
              "IPY_MODEL_9c487735b9c84b8e9707e442552e3476"
            ],
            "layout": "IPY_MODEL_5b2f0489be184f2696c9744530ad68b5"
          }
        },
        "c44305f42df3435f9145c6dc30adbe07": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c603036b0a934c3393129811315362b8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cc0d0c10fab04baa9c346d1eb6e7c1a3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d5cf9cc6b5f340e392b434671c024c0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_33177bf842ee4ff98da93440a6eb78d0",
            "max": 334276,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7b2037c837d94d1a8734e7d8ac458cf9",
            "value": 334276
          }
        },
        "e83d8e0b16364d8b8b8c3975c33f1c43": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ea3d49ff563442fb8919b475bb05cbf0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_73958a2143e64052b16908e7af530ca4",
            "placeholder": "​",
            "style": "IPY_MODEL_8474896ba19c49d886883c8561a50796",
            "value": "Tokenizing data: 100%"
          }
        },
        "f84f041bf88b40829fd261332ea3ff99": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cc0d0c10fab04baa9c346d1eb6e7c1a3",
            "max": 167661,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b5399d6e2d13422497aa4a3f0ea81762",
            "value": 167661
          }
        },
        "f949f184b5d04853a86d3fdcbb3ca7fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5c87632296624fc78b71f8a82e1b2f7d",
            "max": 334276,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3ea78bff89b142dbb454a476bda51ae9",
            "value": 334276
          }
        },
        "fb84f72a0cea42b083cccdca84c6654b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
